---
layout: post
title:  "CS224d Deep Learning for Natural Language Processing"
excerpt: "CS224自学笔记。还在学习更新较慢= ="
date:   2018-08-16 12:12:54 +0000
categories: Notes
comments: true
---

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

*顺手同时开个坑*

# CS224

## NLP - Introduction

（这个课学起来很**，每天只能啃一点）

NLP和计算语言学是同义的，是计算机科学家和语言学家起的不同名字。这是一个跨多个领域的学科，包括计算机科学、语言学以及人工智能的相关领域。我们的目标就是让计算机理解人类语言，并能使用人类语言做一些聪明的决策或事情，或者使用人类语言进行表达。也因此，NLP被认为是AI的一部分。当然AI包括很多部分，例如计算机视觉、机器人、知识表达和推理等，但是语言是一个较为特殊的部分，因为语言是人类一个较为特有的能力，即我们人类的大多数活动都是基于语言的，思考、行动大多离不开语言。相反，地球上很多生物都有较为发达的视觉系统，但只有人类有发达的语言。因此NLP被认为是AI的一个关键部分。在AI的很多方向中，NLP都是基础。当前已有的一些手机语音工具（Siri，Cortana，Google Assistant）来看，我们很容易发现语言是多么重要的一部分。

NLP另一方面也在推动经济发展，例如Amazon的语音控制、Apple Siri，这些都将NLP技术抬升到了消费级技术。

此次课程不会涉及较多的语言学内容，只是从计算机角度去讲解NLP。一般来说NLP分为以下几个层次，并列为输入层的是

1. Speech。通过语音的识别进行处理，其中包括拼音或音标音韵的分析。
2. Text。通过文字的处理，其中包括OCR技术和符号化。

经过处理后以上内容会经过词语结构形态学分析、句法结构分析、语义解释（词句）、话法处理（上下文）本次课程中主要涉及语音输入和句法语义的相关内容。实际上对语音信号的处理正是深度学习崭露头角的领域。

NLP的简单应用领域包括拼写检查、输入法联想，但这些都是最简单最基础的应用，现在我们往往希望训练计算机处理更为复杂的语言理解任务。这里说的更为复杂的任务就是，令计算机阅读文本信息，从报纸、互联网等其他来源获取信息、理解信息、提取需要的特定信息等，或其他分类聚类任务。甚至，我们希望计算机完成所有层级的人物，例如机器翻译、口语会话、构建知识库等。事实上我们在每一次使用搜索引擎的时候，都会有nlp技术在工作，例如拼写修正，智能匹配广告等。不过这些都是开端，目前的更大规模的NLP技术已经在商用上取得了突破性进展，语音识别也有了巨大突破，情感分析产品也在逐渐产生，新闻文章的自动分析、聊天机器人等都在称为巨大新兴产业。

## Human Language

人类语言有什么特殊之处呢？

大多数情况下人们只是收集数据处理数据，但是人类语言并不是一些随机信号，人类语言中总包含着人类需要表达的某些信息，是人类传递信息的工具。人类语言是一个复杂的系统，但是年幼的小孩却能张口就说出。

人类语言还是一个离散的、符号化的、明确地信号系统。我们有表明某些概念的词汇，例如火箭、小提琴，而实际上我们也是使用符号在和其他人交流。另外我们可以用这些符号表达情感。符号不知是基于逻辑的或传统AI的发明。当人类使用语言进行交流的时候，尽管我们想要交流的内容包含符号，但沟通的具体方式依靠了连续的载体，而不同的连续载体可以用来传递完全相同的信息，例如声音、手势、图像等，在不同的符号后似乎存在一种特殊的编码。语言传递信息的过程似乎就是连续信息$$\rightarrow$$离散的符号$$\rightarrow$$连续的信息，语言只是一种离散的工具，而处理这些信息的正是我们处于连续工作状态的大脑，实际上载体也是连续的。

## Deep Learning

语言的内容还很多，需要的话可以自行学习语言学的相关内容。

深度学习在近年间突然火爆，扩展到了多个领域，日新月异。实际上深度学习是[机器学习](https://psycholsc.github.io/notes/2018/08/10/CS229-Machine-Learning.html)的一个分支。和传统编程不同，我们并不告诉机器我们要做什么，不会对任务进行编程，而是使用学习算法使计算机从经验中学习。当然深度学习还是与传统的机器学习不同的。

传统机器学习都是围绕决策树、逻辑回归、朴素贝叶斯、SVM等算法展开的，这些算法都还是由人们来对模型进行分析，确定了较为主要的特征后进行编程的算法，类似于我们所说的监督学习的分支。这类机器学习算法个改良往往还是通过人为添加特征量进行的改良。从无监督学习的角度来看，监督学习的机器学习到的内容什么都不是。监督学习实际上拥有已经匹配的特征，经过数据的训练从而获取正确的模型的权重。相反，设计相关算法与特征的人却需要花上一定的时间与精力去学习。所有消耗资源的部分就是计算机的优化器。例如线性分类器，计算机只是去优化调校权值数字罢了。这是电脑比较“擅长”而人类不擅长的运算过程。但是这不应该是机器学习的全部。

深度学习是表征学习（representation learning）的一个分支，理念是我们只向电脑提供最原始的信号，然后让电脑去学习并确定什么特征较为重要，依此来完成任务。所以深度学习的意义是自动获取多层的特征，并通过这种方式获得比人类自行设计模型更好的结果。这大概可以看做是无监督学习的一种表现。现在，大约一半的情况下，深度学习只是指在利用神经网络，另一半的情况下就是记者在写报告时瞎写（这个教授太耿直了）。

现在使用神经网络是深度学习的一个主要方法，课程也会讨论使用不同的神经网络来实现不同的算法。当前人类构建的神经网络往往是复杂的神经元的不同堆栈形式，最终导致了某种特定的“思维”或“行为”，这个和统计学家使用神经网络的初衷是不同的。有关神经网络在深度学习中的历史的内容，[这里有一篇PDF格式的论文专门描述](https://github.com/psycholsc/psycholsc.github.io/raw/master/assets/Deep%20Learning%20in%20Neural%20Networks_An%20Overview.pdf)，原地址为[https://arxiv.org/abs/1404.7828](https://arxiv.org/abs/1404.7828)，作者是[Jürgen Schmidhuber](https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber)，实话讲我似乎见过这个人（真人）。

一般来说我们手动设计的特征往往太过于具体，且不完整，需要很长时间来验证测试，而且最终只能达到一定的表现水平，但是自行学习到的特征往往适应性强，训练快，而且可以不断提升表现水平，甚至超越人类，因此深度学习提供了一个灵活通用的学习架构，可以表示各种类型的信息。可以用于监督学习（自定义标签）和无监督学习（从最原始的信息开始）。

如果深度学习没有效果，也不会有那么多人进行研究。大约从2010年开始，深度学习的算法就要远优于其他任何一种传统的已经沿用近30年的机器学习算法，但不仅如此，近年间深度学习发展迅速，改进和提升的速度非常迅速。但是实际上我们使用的很多深度学习算法都是上世纪八九十年代左右的时候提出的，但是受限于当时的技术水平，例如编程工具、计算水平、数据量等技术上的局限，这些方法在当时并没有什么效果。但是现在我们几乎有了这些所有的优势。当然算法的革新进步也是不可忽视的。

深度学习真正意义上的第一个重大突破是语音识别。人们用了几十上百年的时间，只对这个算法提升了几个百分点，而当换用深度学习模型的时候，识别错误率直接下降了30%。

而第二个突破就是计算机视觉，和语音识别与处理几乎是现在深度学习的两大领域。在ImageNet 计算机视觉竞赛中突然异军突起得到高分的CNN基础的模型，就是应用深度学习的模型。相关内容在CS231中会有涉及。

## Requirements

1. Python。作业会以Python的形式给出。对TensorFlow的用法有要求。
2. 多变量的微积分，线性代数（大一结束的中国学生就能掌握）
3. 基础概率论和数理统计（大二的学生就能掌握）
4. 机器学习的基础知识（[CS229课程笔记](https://psycholsc.github.io/notes/2018/08/10/CS229-Machine-Learning.html)在同步更新）

## Destination

1. 理解并有能力运用有效的现代方法进行深度学习 - 课程涵盖所有的基础知识和NLP的主要方法，RNN，attention等
2. 对人类语言有总体了解，以及目前人们理解和产生语言所遇到的困难。
3. 具有为重要NLP问题构建系统架构的能力（作业见）

课程总计三个大作业（不出意外的话我应该会更代码，一个最终项目，自己提出或导师给出）

## Difficulty

人类语言总是模棱两可的，但编程语言往往都是设计之时就十分明确了，有固定的格式，固定的写法，人类语言总是不按语法或规则来。我们往往需要通过意思或环境来进行判断句意。指代关系也不像编程中那么具体那么明确。语言作为最有效的通信工具的同时，人们往往倾向于省略很多词汇，但是受众往往会自己补全；而在程序编写的时候我们会明确表达出它运行所需要的一切。

![i_could_care_less](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/i_could_care_less.png)

教授给出了几个语义歧义的例子

1. The Pope‘s baby step on gays.
2. Boy paralyzed after tumor fights back to gain black belt.
3. Scientists study whales from space.
4. Juvenile Court to Try Shooting Defendant



## Deep NLP

融合了Deep Learning的NLP。当前深度学习已经被应用于了语言学的各个方面，层级涉及语音文字句法语义。目前常用的手段都是使用几个核心工具箱来解决所有的NLP问题，不需要针对不同任务做定制，而且工作效果很好。深度学习和语言学都是以词义为起点，我们要做的事，也就是将一个词汇用一个向量来表示，例如

$$ expect=\left[ \begin{matrix} 0.286  \\ 0.792  \\ -0.177 \\ -0.107 \\ 0.109 \\ -0.542 \\ 0.349 \\0.271 \\0.487  \end{matrix} \right] $$

这就是对单词expect的向量表示，但是一般的词向量可能至少会使用25维向量，如果我们真的要做一些事情，我们可能需要设置300维向量，如果我们手中有最优秀的设备，我们可能会使用到1000维向量。当我们把词向量放到高维向量空间时，我们会发现这些空间成为了非常棒的语义空间，具有相似含义的词汇将在矢量空间中聚类，向量的方向会包含词语的成分和意义信息。但是人类并不擅长处理高维空间的问题，让人类去设想四位及以上的空间几乎都是不可能的，所以我们会在处理给人类观察的时候将其映射到二维或三维的空间中，所以我们看到的单词云往往都是对词向量的二维投影，只有一定的参考意义。

> 向量空间的二维投影的坐标轴是没有意义的，特征量是深度学习自己得到的，经过映射后也不存在什么较为明显的意义。

传统意义上的处理会认为复合词汇应该被拆分理解，认为他们是由更小的单元词素组成的。深度学习中我们会将复合词的各个部分看成不同的向量，我们要建立一个神经网络，用小块的单元构建更大的意义单元。

有关句义理解，在深度学习上的思路是完全不同的。传统方法利用的是lambda演算、传统的语义理论，对每个单词手动赋予语义函数，并用一些方法将语义组合，分析句意。但是我们从一开始就用向量来表达词汇，因此思路是完全不同的。

另外对于机器翻译等问题，基于RNN的深度学习的机器翻译效果我们已经可以看到。

使用向量的方法十分微妙灵活，我们在后面会慢慢讲。



---

以上都是第一节概述课，接下来开始分节学习正式课程

---

  

##Word Vectors

（这个网站的字幕简直有毒=-=）

我们希望使用一组数值组成的词向量来描述一个词汇的词义，但这实际上是一个有一定争议的做法。

在标准的语言学中，我们认为单词就像是一种符号，指代了实际上的一些具体事物或含义，当我们查阅“meaning”的含义的时候，我们可以看到，它的意思并不很容易在计算机中进行表达。几十年内的处理方式都是采用分类的方式进行处理。nltk这个库是目前Python处理自然语言的主要库，

{% highlight python %}

from nltk.corpus import wordnet as wn
panda=wn.synset('panda.n.01')
hyper=lambda s:s.hypernyms()
list(panda.closure(hyper))

{% endhighlight %}

这段代码可以查看‘panda’这个词的上位词，我们可以看到食肉、哺乳动物等输出结果。

WordNet对good的词义解释很多，各种含义上的解释。而实际上这些解释并不完美，因为这些解释在含义上有轻重等不同区别，只有人才能明白其中的含义差别，这并不利于机器对词义的理解。使用修辞手段也会让WordNet查询不到词义。人们在定义同义词的时候是非常主观的非常模糊的，而且需要大量人力的多年努力才能完成这样的工作。几乎所有的NLP都是用了“atomic symbol”来表示单词，表示不可再分的单词，但是从神经网络的角度去理解，在词向量空间中，这种原子符号相当于一个位置是1，其他位置都是0的一类向量，这叫做“one-hot encoding“。这种表示方法并没有揭示词汇之间的关系，这是从一个角度去彰显旧算法的失误之处。例如

$$motel=[0 \:0 \:0\:0\:1\:0\:0\:0\:0\:0\:0 ]$$

$$hotel=[0 \:0 \:0\:0\:0\:0\:1\:0\:0\:0\:0 ]$$

我们看不出两者有任何联系，但实际上二者是一个同义词。

如果要这样处理，那么很多名词都在原子层面，有时识别器本身也只有20000多词的词汇量，而在做机器翻译时我们可能会有一个50万词的词汇表，谷歌也发布了一个有着1300万词汇的词汇表，用传统算法来表示的时候，我们的向量就会很长很长，不利于处理。

我们自然就希望处理成其他形式的编码，使得我们能够直观的看出词汇之间存在联系，然后比如利用点积来判断两个词汇的相似性等。

这时我们会需要使用向量来描述词汇，让机器使用大量的语料去理解每一个词汇的含义。

下面介绍word2vec软件包中的skip-gram模型，[这里有一篇文献专门用于介绍skip-gram模型](https://arxiv.org/abs/1301.3781)

![skip-gram](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/skip-gram.png)

skip-gram做的工作就如上面所说，总的来说是一个非常精巧的模型。模型首先需要一个词汇表，假设我们的语料库中有10000个单词，那么我们的模型输入就是10000个量，全部都编码为前面所谓的“one-hot encoding”，即这个向量只有一位是1，其余位都是0，且1的位置各自不同。比较简单的方式就是直接输入10000维的单位矩阵。

将这个单位矩阵输入后，要与某个权值矩阵$$W$$求积，得到一个输出，这个输出层被称为隐藏层。隐藏层通过一个Softmax层得到最终输出，这个输出是概率分布。注意Softmax，Softmax是一个多分类器，其定义如下

对于有$$n$$个元素的数组$$V$$，

$$S_j=\frac{e^{V_j}}{\sum\limits_{i=1}^{n}e^{V_i}}$$

这就是第$$j$$个元素的Softmax。这个函数的最终会做归一化（详见分母），这样输出结果就会是一个相对概率。另外仔细看这个定义可以发现是对结果求指数后的归一化，显然原来越大的项，在指数归一化之后就会变得更大，从而使输出结果更接近“one-hot encoding”。













好的下面会去更新一段时间CS229。

====下面都是看视频编的过几天再改====

skip-gram模型的思想是，在每一个估计步都取一个词作为中心词汇（center word），然后尝试去预测它一定范围内的上下文词汇，模型将定义一个概率分布，即给定一个中心词汇，某个单词在他上下文的概率。我们会选取词汇的向量表示，以让概率分布值最大化。我们要明白这个模型**只有一个概率分布**，并不是说对中心词汇左边的词汇和右边的词汇分别有一个概率分布。这个概率分布就是输出。接下来定义一个半径$$m$$，然后从中心词汇开始，到距离为$$m$$的位置，来预测周围的词汇。我们会在各处进行多次重复操作，我们要选择词汇向量，以便让预测的概率达到最大。因此损失函数表示的是，我们拿到一段很长的文本，有了足够长的词汇序列，然后遍历文本中的所有位置，对于文本中的每个位置，我们都会定义一个围绕中心词汇的大小为$$2m$$的窗口（中心词前后各$$m$$个单词），这样就得到一个概率分布，可以根据中心词汇给出其上下文的次会出现的概率。然后我们设置模型的参数，让上下文所有词汇出现的概率都尽可能的高。这个模型中唯一的可变参数就是$$\theta$$，表达式如下

$$J(\theta)=-\frac{1}{T}\sum\limits_{t=1}^T\sum\limits_{-m\le j \le m ; j!=0}logP(w_{t+j}|w_t)$$

$$J'(\theta)=\prod\limits_{t=1}^T\prod\limits_{-m\le j \le m}P(w_{t+j}|w_t;\theta)$$

对于人为设定、不跟随程序训练结果变化而变化的参数被称为超参数（hyper parameters）。窗大小$$m$$就是一个超参数。

====算了我看完之后再编吧====





{% if page.comments %}

<div id="container"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
var gitment = new Gitment({
  id: '22', // 可选
  owner: 'psycholsc',
  repo: 'temp',
  oauth: {
    client_id: '9183e7259ea6d850a7df',
    client_secret: 'd0a82473ca685629b50ded0553f402b6ba2b2dee',
  },
})
gitment.render('container')
</script>

{% endif %}