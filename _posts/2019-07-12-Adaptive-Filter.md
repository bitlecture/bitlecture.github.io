---
layout: single
comments: true
title:  "自适应滤波器"
excerpt: "-"
date:   2019-07-12 12:42:24 +0000
categories: Notes
tags: "复习"
toc: true
toc_label: "Table of Contents"
toc_icon: "cog"
author: Shicong Liu
---



---

[TOC]

`2019-7-12 14:36:16`

许文龙老师是个好老师。考试的时候要求复述提前阅读的一篇文献。这里可以是许多自适应相关的内容。

`2019-5-12 13:55:15`

重新校对了部分错误。如果手里有课本的话，建议直接看复习系列的内容。

## 前言

这个课还是有一点难度的，不过看起来讲的内容没有整本书平均难度那么高。

下面的顺序可能不是完全按照课本的顺序，讲的内容也并非全面。最后的一部分内容是复习总结的。

## 自适应滤波器基础

自适应滤波的基本原理是，设计一个参数可调的滤波器（例如FIR横向滤波器），然后设计自适应算法，根据目的信号和输出信号之间的差值的某种函数关系，最小化这个目标函数以达到误差最小化，调整可调系数。

因为在环境中可以自行调整参数的变化，因此我们说这时一种自适应算法、自适应滤波器。

首先可实现的数字滤波器我们一般认为有两类，即`IIR`滤波器和`FIR`数字滤波器

两类滤波器得名都是因为其结构特点的，由于`IIR`存在较为复杂的反馈结构，使其能够产生无限长的响应，而同理，由于`FIR`滤波器并没有反馈结构，因此并不能产生无限长的响应。这两类滤波器的结构都是固定的，其中的参数也是经过设计得到的

`IIR Filter`
$$
\begin{equation}
\begin{split}
H(z)=\frac{\sum\limits_{k=0}^Mb_kz^{-k}}{1-\sum\limits_{k=1}^Ma_kz^{-k}}
\end{split}
\tag{1}
\end{equation}
$$

可以看出是存在反馈结构的。这里引用一段其他地方的定义，`IIR`滤波器也被称为**递归**滤波器，从命名来看就是针对含有反馈结构的。我们常说这两类滤波器很大程度上是因为这两种滤波器可以写出封闭函数结构，其中`IIR`滤波器由于采用反馈电路设计，会导致误差不断积累而产生寄生振荡。这两类滤波器设计都可以参考成熟的模拟滤波器设计原理。不过由于`IIR`滤波器的相位特性比较难以控制，一般在宽频带内需要相位校准网络进行相位校准。

`FIR Filter`
$$
\begin{equation}
\begin{split}
H(z)=\sum\limits_{k=0}^Mb_kz^{-k}
\end{split}
\tag{2}
\end{equation}
$$
这是有限长度单位冲激响应的滤波器，一看就是一个线性延迟组合的结构，是非递归结构。`FIR`滤波器设计可以充分考虑到相位的问题，因此很有利于设计线性相位的滤波器，这一点十分优秀。而由于不存在递归结构，因此响应长度有限，也不会出现`IIR`那样的误差响应积累效应。处理信号的时候，往往就是一个模数转换器，对信号采样后送入延迟单元入口，后面就是`FIR`的处理了。

但是如果处理自适应问题，仅仅是这样的非时变滤波器往往是不能满足要求的，因为在我们处理的时候，不同场景往往需要不同的滤波器。比如在移动通信中，信道是时刻变化的，对于这种情况，我们的数字滤波器必须能够处理所有情况的问题，因此设计自适应滤波器就可以解决已知或未知信道的问题。

这是一种时变滤波器。

---

所谓**自适应滤波**，就是根据预先确定的性能准则，设计结构预算法，实现参数自适应调整的滤波器。这种滤波器并没有明确的设计规范，但是一定会要求期望或参考信号。一般来说框图可以这样表示

<div style="text-align:center"><img alt="" src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/AF.png" style="display: inline-block;" width="500"/>
</div>

对于输入信号$$x(k)$$，我们滤波后得到输出$$y(k)$$，通过与误差信号进行比较后反馈差异，利用该结果 实现参数自适应调整。首先是自适应信号处理的三个基本要素

- 应用
    - 根据具体环境选取输入信号与期望信号
- 自适应滤波器结构
    - `IIR`，实现、分析简单，有极点
    - `FIR`，全零点无反馈有唯一最优解
- 算法
    - 实现预先确定的目标函数的最小化
    - 采用搜索算法或者其他优化方法；目标函数和误差信号的特点也能计算。

另外对于自适应算法，也有三个重要要素

- 最小化算法
    - 牛顿法，需要计算逆矩阵，该操作较慢。但是收敛快。
    - 准牛顿法，估计逆矩阵。
    - 梯度下降法，利用梯度搜索最优解。计算最为简单。
- 目标函数形式
    - 均方误差（`MSE`）
    - 最小二乘（`LS`）
    - 加权最小二乘（`WLS`）
    - 瞬时平方值（`ISV`）
- 误差信号表示
    - 这有很多种，会影响到计算复杂度、收敛速度以及鲁棒性等。

简单说一下应用。自适应滤波常见应用是，**系统辨识**、**信道均衡**、**信号增强**、**信号预测**。

## 维纳滤波器

维纳滤波器是本课程中介绍的第一个重要的自适应滤波器，命名大约是因为其提出者的名字，`Norbert Wiener`。这个是上世纪四十年代末提出的一种自适应滤波器，其优化准则是**最小化均方误差**。是一种线性滤波器（实际上就常常采用`FIR`结构实现）。首先我们介绍两种简单的滤波器结构，由于是本科生课程，此处只研究到线性滤波器的简单实现。

**1. 线性组合器**

线性组合器就是按照不同权重将不同信号进行线性加权的设备。实在是不想解释了。根据其计算特点，输出可以表示为



$$
\begin{equation}
\begin{split}
y(k)=w^T(k)x(k)
\end{split}
\tag{3}
\end{equation}
$$


这里$$x(k)$$是不同信号

**2. FIR横向滤波器**

嘿呀这个好像也没什么说的必要。横向滤波器的结构有机会的话这里补一张图说明一下。实际上是同一个信号的不同延迟送入线性组合器中，写成表达式也是


$$
\begin{equation}
\begin{split}
y(k)=w^T(k)x(k)
\end{split}
\tag{4}
\end{equation}
$$


这里的$$x(k)$$是同一信号的不同延迟。

---

然后此处介绍信号的相关矩阵。信号的相关矩阵一般定义为



$$
\begin{equation}
\begin{split}
R=E \left[ XX^H \right]
\end{split}
\tag{5}
\end{equation}
$$



出来是一个矩阵。该矩阵是一个半正定矩阵，且是一个厄米特矩阵，满足共轭转置等于自身。

关于特征值和特征向量，这里给出解释（参考资料为同济大学《线性代数》）

如果满秩矩阵$$R$$特征值是$$\lambda_i$$，那么必然存在以下关系


$$
\begin{equation}
\begin{split}
diag\{ \lambda_i \}=\Lambda=P^{-1}AP
\end{split}
\tag{6}
\end{equation}
$$

一定能找到一个矩阵$$P$$使上式成立，这个矩阵正是按特征值顺序排列的特征向量。

二次型


$$
\begin{equation}
\begin{split}

f(x)=x^T Ax

\end{split}
\tag{7}
\end{equation}
$$

假如此时$$x=Cy$$，那么



$$
\begin{equation}
\begin{split}

f(x)=y^TC^T A Cy=y^T \Lambda y

\end{split}
\tag{8}
\end{equation}
$$


若$$A$$是一个对角阵，则$$C$$是一个正交矩阵，满足$$C^T AC=C^{-1}AC$$

- 待续

另外这些特征值有功率的含义。

维纳滤波器自己要会推

维纳解$$w_0 =R^{-1}p$$

最小化目标函数时，误差信号与输入输出信号正交。如果均值为零就是不相关信号了。

- 在统计平均下，维纳滤波器的准则是滤波器输出与期望响应之间的误差的均方值最小。
- 最小均方误差准则得到的是具有相同统计特征的一类数据的最佳滤波器。
- 如果输入信号与期望响应联合平稳且各自平稳，所得的最佳滤波器为维纳滤波器。

---

既然说了不能复刻，就说点不一样的东西好了。第二章中有一部分老师其实并没有讲，但是我一看还是蛮重要的。

## 应用简述

### 系统辨识

系统辨识的结构如图

<div style="text-align:center"><img alt="" src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/2.10.si.png" style="display: inline-block;" width="500"/>
</div>

如图所示输入一个信号分别进入两个系统中，最终让这个自适应滤波器拟合这个系统的特点，进行辨识。对这类问题的处理，我们一般采用假设系统响应为$$\boldsymbol h$$，误差信号就是
$$
\begin{equation}
\begin{split}
e(k)=& d(k)-y(k)\\=& \sum_{l=0}^{\infty}h(l)x(k-l)-\sum_{i=0}^Nw_i(k)x(k-i)
\end{split}
\tag{1.1}
\end{equation}
$$
这个理解起来可能会有一定困难。前面是系统的输出响应，这个就是简单的卷积操作，毕竟冲激响应是一个定值；后面是滤波器的计算，我们这里的$$k$$是迭代次数，标记系数位置的量是$$i$$。相当于也是一个卷积，但是其取值是随着时间进行更新的。

假设输入是一个白噪声，`MSE`就可以计算为
$$
\begin{equation}
\begin{split}
\xi\:=&\:E\left\{ [\boldsymbol h^T\boldsymbol x_\infty(k)-\boldsymbol w^T\boldsymbol x_{N+1}(k)]^2 \right\}\\ \:=&\:\sigma_x^2\sum_{i=0}^\infty h^2(i)-2\sigma_x^2 \boldsymbol h^T \left[ \begin{matrix} \boldsymbol I_{N+1}\\\boldsymbol 0_\infty \end{matrix}\right]\boldsymbol w+\boldsymbol w^T \boldsymbol R_N\boldsymbol w
\end{split}
\tag{1.2}
\end{equation}
$$
如果计算导数也可以得到最优匹配为$$\boldsymbol w_0=\boldsymbol h_{N+1}$$，这个是$$N$$阶自适应滤波器的极限，其中后面这个$$\boldsymbol h_{N+1}$$是指$$\boldsymbol h$$的前$$N+1$$项，后面补零。

对于没有测量噪声或信道噪声的环境来说，如果未知系统冲激响应为有限长且自适应滤波器建模充分，那么最后的`MSE`可以为$$0$$。不过显然这个是不能避免的，因此结果总会有噪声的方差项。实际应用很多。

### 信号增强

滤波器常用的领域往往自适应滤波器也是常用的。信号增强的框图如下

<div style="text-align:center"><img alt="" src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/2.10.se.png" style="display: inline-block;" width="500"/>
</div>

在信号增强中，参考信号是受到加性噪声$$n_1(k)$$污染的信号，如图所示，而自适应滤波器的输入是另一个噪声信号$$n_2(k)$$。我们说这个噪声是与上一个噪声信号相关的，但是与$$x(k)$$无关。这种结构常被用在电源线干扰消除等应用中，在通信系统中消除杂波引起的回音也被认为是增强问题。此时误差信号就是
$$
\begin{equation}
\begin{split}
e(k)=& d(k)-y(k)\\=& x(k)+n_1(k)-\sum_{i=0}^N w_1n_2(k-l)=x(k)+n_1(k)-y(k)
\end{split}
\tag{1.3}
\end{equation}
$$
此时的`MSE`为
$$
\begin{equation}
\begin{split}
E[e^2(k)]=E[x^2(k)]+E\{ [n_1(k)-y(k)]^2 \}
\end{split}
\tag{1.4}
\end{equation}
$$
由于待增强信号与另一路噪声无关，因此认为这路噪声经过线性滤波后与待增强信号仍然无关。这样就可以轻易地消除掉交叉项。

如果我们将$$n_2$$作为输入信号，就可以通过某种方式预测到$$n_1$$，这取决于两者相关性。这样就能最大化输入信号，此时我们最小化`MSE`为
$$
\begin{equation}
\begin{split}
\xi_{min}=E[x^2(k)]
\end{split}
\tag{1.5}
\end{equation}
$$

### 信号预测

一般来说信号预测的滤波器结构为

<div style="text-align:center"><img alt="" src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/2.10.sp.png" style="display: inline-block;" width="500"/>
</div>

显然可以知道`MSE`
$$
\begin{equation}
\begin{split}
\xi=E\{ [x(k)-\boldsymbol w^T \boldsymbol x(k-L)]^2 \}
\end{split}
\tag{1.6}
\end{equation}
$$
最小化这个值可以得到一个`FIR`滤波器，可以通过过去的样本值预测当前样值。如果我们将`MSE`优化到足够小的水平，我们甚至可以认为这个`FIR`滤波器就是信号的模型。最小`MSE`的理论值为
$$
\begin{equation}
\begin{split}
\xi_{min}=r(0)-\boldsymbol w^T\left[\begin{matrix}r(L)\\r(L-+1)\\ \cdot\\\cdot \\r(L+N)\end{matrix}\right]
\end{split}
\tag{1.7}
\end{equation}
$$

### 信道均衡

上学期通信原理就学过信道均衡这个东西，当时用的就是横向滤波器，最简单的`FIR`结构。自适应信道均衡如下

<div style="text-align:center"><img alt="" src="https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/2.10.ce.png" style="display: inline-block;" width="500"/>
</div>

这个就是模拟了一个信道模型和一个直接延迟模型，设计延迟是因为`FIR`滤波器阶数导致的必要的延迟。

## LMS算法

这里的排版比较乱，很多东西现在还没有想好应该怎么叙述才比较好，所以很多东西只是直接贴出来，以便以后排版。

我们采用维纳滤波器是为了最小化均方误差，凡是最小化均方误差的滤波器实际上都是维纳滤波器。但是维纳滤波器在实践上有几个不能实现的地方，例如

- 维纳滤波器采用的是均方误差，但是实际应用中我们根本得不到统计平均值，只能通过测量得到当前值。
- 无法计算统计平均，只能利用当前值，就需要使用估计值替代。涉及的量有相关矩阵和我们定义的向量$$p$$

$$
\begin{equation}
\begin{split}
\hat {\overrightarrow R}(k)=\overrightarrow x(k)\overrightarrow x^T(k)\\
\hat {\overrightarrow p}(k)=\overrightarrow d(k)\overrightarrow x(k)
\end{split}
\tag{9}
\end{equation}
$$

以当前次的估计值代替实际值。而我们采用的最陡下降法，计算时

$$
\begin{equation}
\begin{split}
w(k+1)=w(k)-\mu \hat g_w(k)
\end{split}
\tag{10}
\end{equation}
$$

按照梯度的反方向进行更新，每次更新步长为$$\mu$$。如果错误仍然定义为$$e=d-y\:$$这样的方式，则错误平方的梯度估计值可以写作

$$
\begin{equation}
\begin{split}
\hat g_w(k)=-2e(k)x(k)
\end{split}
\tag{11}
\end{equation}
$$

注意这个估计值是通过直接带入$$g_w(k)=-2p(k)+2R(k)w(k)$$，这个式子实际上就是我们在推导维纳解的时候通过对`MSE`求导得到的。

如果我们对这个估计值进行相同的求导我们会发现结果是一样的。实际上我们每次迭代时的均方误差与瞬时误差是**等价**的。*其实话不能这么说，因为等价关系并不显然，通过MSE计算的结果实际上是不包含k的，这里单独计算一次的，我们应该说形式上是完全相同的。*

那么把这个带进去就可以得到梯度更新公式
$$
\begin{equation}
\begin{split}
w(k+1)=w(k)+2\mu e(k)x(k)
\end{split}
\tag{12}
\end{equation}
$$
这个通过瞬时值而不是统计平均值方法得到的计算结果被称为`LMS`算法。这个平均的思想其实我并不知道在哪里。但是**最小**和**平方**都是很显然的。这种方法实际上只需要对反馈结果做一个相关延迟这样的处理，就可以实现自适应滤波。

### 梯度特性

前面介绍的，`MSE`方法搜索梯度方向为
$$
\begin{equation}
\begin{split}
g_w(k)=2[Rw(k)-p]
\end{split}
\tag{13}
\end{equation}
$$
而在`LMS`中，我们的梯度采用估计值
$$
\begin{equation}
\begin{split}
\hat g_w(k)=2[x(k)x^T(k)w(k)-d(k)x(k)]
\end{split}
\tag{14}
\end{equation}
$$
实际上这两个方向是很不相同的，但是从平均的意义上来看。对于固定抽头的滤波器（或者如果认为滤波器抽头系数与信号无关），此时对估计梯度值求统计平均就会得到`LMS`梯度方向的统计平均就是真正的梯度值。我们可以把这个梯度的估计当做是真实值的无偏估计值。

### 收敛特性

我们这里主要介绍要如何理解计算过程。我们假设测量过程中存在测量白噪声$$n(k)$$，那么在计算的时候假设误差信号与输入信号是独立的，就能在期望中去除一项，只留下$$w$$项计算。

通过某种对角变换，我们能够将原来的相关矩阵变换为对角阵，通过一个统计平均的方式进行计算，可以得到某个形如$$I-2\mu R$$或者$$I-2\mu \Lambda$$的系数矩阵。这个系数往往有$$k$$次方，这时就要求这个系数矩阵的取值小于$$1$$，由此可以得到
$$
\begin{equation}
\begin{split}
\mu <\frac{1}{\lambda_{max}}
\end{split}
\tag{15}
\end{equation}
$$
这个取值可以保证**结果**是收敛的，不过是一个上界，实际取值只要小于这个上界就可以了。

这里说的**结果**是滤波器系数。

### 系数误差向量协方差矩阵

这里实际上是推导二阶统计量。我们先看一下系数误差向量的协方差

> 系数误差向量，实际上就是$$\Delta w(k)$$，这个定义为当前系数与维纳解系数之间的误差。如果我们观察这个量的协方差，就可以观测收敛过程的方差变化

$$
\begin{equation}
\begin{split}
con[\Delta w(k)]=E[\Delta w(k)\Delta w^T(k+1)]
\end{split}
\tag{16}
\end{equation}
$$

带入上面的计算结果，$$\Delta w(k+1)=[I-2\mu x(k)x^T(k)]\Delta w(k)+2\mu e_0(k)x(k)$$，考虑到最佳误差与输入信号之间的独立正交关系，可以去除相关项，最后可以得到，当$$k$$无限增大的时候，由于其中可见的激励项的存在，方差永远也不可能趋近$$0$$。

不过由于是最优化算法，人们一般喜欢将性质特殊的厄米特相关矩阵对角化进行计算，主要方法就是左右乘矩阵$$Q^T$$和$$Q$$。这里的稳定性条件是`MSE`的收敛，或者说代价函数取值最终是收敛的。
$$
\begin{equation}
\begin{split}
0<\mu<\frac{1}{2\lambda_{max}+\sum_{j=0}^{N}\lambda_i}<\frac{1}{\sum_{j=0}^N\lambda_i}<\frac{1}{tr[R]}
\end{split}
\tag{17}
\end{equation}
$$
实际应用中往往采用的是最后一个简化的表现形式。

### 误差信号的特性

如果测量噪声是加性噪声，我们就可以在实际测量结果后面直接叠加一个$$n(k)$$。这个结果实际上影响不大，因为后面的相乘只要涉及统计平均就会被消除。我们这里主要希望讨论的问题是，如果我们使用的是`FIR`自适应滤波器辨识一个`IIR`系统，会发生什么样的事情。实际上此处没加证明地认为，如果`FIR`建模不足，就会出现残留误差，即因为`FIR`点数不够多而只能辨识`IIR`系统的前几个点。

我们认为结果是这样的
$$
\begin{equation}
\begin{split}
E[e(k)]=E\left[ \sum_{i=N+1}^{\infty}h(i)x(k-i) \right]+E[n(k)]
\end{split}
\tag{18}
\end{equation}
$$
前一部分是目标信号中未被辨识的部分，如果输入信号的均值为$$0$$，那么我们就可以认为上述结果为$$0$$。即如果我们输入信号是一个均值为$$0$$的信号，那么我们认为`FIR`的估计是无偏的。（这么说对么）

### 最小均方误差（实际上是二阶统计量）

上一部分计算了误差信号的一阶统计量，即均值，此处要计算二阶统计量，均方误差`MSE`了。计算方式就是$$E[e^2(k)]$$

我们针对建模不充分（即上述的条件）与存在加性噪声的环境下。我们认为实际上参考信号可以表示为$$\boldsymbol h^T\boldsymbol x_\infty(k)$$。实际上这里的$$\boldsymbol h^T$$是一个无限长的函数，而$$\boldsymbol x_\infty$$也是一个无限长序列。我们利用矩阵巧妙将其分离为`FIR`可以识别和不能识别的两部分。计算`MSE`可得，最小均方误差为
$$
\begin{equation}
\begin{split}
\xi_{min}=&E[e^2(k)]_{min}=\sum_{i=N+1}^\infty h^2(i)E[x^2(k-i)]+E[n^2(k)]\\=&\sum_{i=N+1}^\infty h^2(i)\sigma_x^2+\sigma_n^2
\end{split}
\tag{19}
\end{equation}
$$

- 当自适应滤波器具有充分的阶数的时候，可以输出一个接近$$d(k)$$的信号，得到最小`MSE`
- 非充分建模的时候，我们会得到额外的`MSE`（如上第一项）以及（测量）噪声的方差

以上两小节就是对误差信号的一二阶统计量的分析结果。

### 超量均方误差与失调

这个翻译的很差劲，中文版的课本一点也看不懂，绝了。这个主要说的是啥问题呢，就是说梯度下降计算的时候，我们的结果可能收敛到维纳解，但也有可能收敛不到。如果收敛到了维纳解，我们就说这个误差是最优误差信号，如果没有，这个误差就会是最优误差信号与抽头偏差导致的误差之和，即
$$
\begin{equation}
\begin{split}
e(k)=e_0(k)-\Delta \boldsymbol w^T(k)\boldsymbol x(k)
\end{split}
\tag{20}
\end{equation}
$$
此时`MSE`的超量是可以计算的。这里计算的思想还需要亲自去体会一下，因为计算过程中涉及特征矩阵的部分全部采用对角变换将其变换成特征值对角阵，此时采用的对角变换阵是特征向量正交矩阵$$Q$$。这里继续计算推导可以得出
$$
\begin{equation}
\begin{split}
\Delta \xi(k)=\sum_{i=0}^{N}\lambda_i\nu_i'(k)
\end{split}
\tag{21}
\end{equation}
$$
这里给出的$$\nu$$其实是一个复杂表达式的简化。带入前面计算过的结果可以知道，超量误差为
$$
\begin{equation}
\begin{split}
\xi_{exc}=\lim_{k\to \infty}\Delta \xi(k)\approx \frac{\mu\sigma_n^2 tr[\boldsymbol R]}{1-\mu tr[\boldsymbol R]}\approx \mu\sigma_n^2 tr[\boldsymbol R]=\mu(N+1)\sigma_n^2\sigma_x^2
\end{split}
\tag{22}
\end{equation}
$$
如果是采用比值的形式，这个量就被称为是失调，结论为
$$
\begin{equation}
\begin{split}
M=\frac{\xi_{exc}}{\xi_{min}}\approx \frac{\mu tr[\boldsymbol R]}{1-\mu tr[\boldsymbol R]}
\end{split}
\tag{23}
\end{equation}
$$
可以看出，如果我们采用的$$\mu$$越小，则最后就会产生越小的失调（或超量误差）。但是这样也会导致收敛更慢。

### 瞬态特征

这里说的是，自适应滤波器的系数收敛是指数衰减的，可以用一个时间常数为$$\tau_{wi}$$的指数包络来近似。这一点课本描述不太好，老师决定现场展示一下。结论是，当我们认为依最慢收敛模式收敛100倍的时候就是完成收敛的时候，迭代次数约为
$$
\begin{equation}
\begin{split}
k\approx 4.6\frac{(N+3)\lambda_{max}}{2\lambda_{min}}\approx 2.3(N+3)
\end{split}
\tag{24}
\end{equation}
$$
多说一句，是这样的，由于特征值的不同，所以我们采用的系数$$\mu$$本应不同。但是由于计算的时候没有考虑这个参量，考虑的时候又有可能导致其他问题，因此一般的采用一个统一值进行计算。我们采用

`2019-3-29 10:49:55`

---

`2019-4-14 22:27:54`

半个月没更新了，其实现在啥也看不懂了。

## 基于LMS的算法

几乎都是以计算复杂度为代价，提升收敛速度。当然也有一个是降低计算复杂度，但是收敛时间是否降低另说。事实证明新的算法确实可以更快地收敛。

### 1. LMS-牛顿算法

牛顿法对信号的二阶统计量进行估计，避免当输入信号的相关性较强的时候收敛减慢。通过增加计算复杂度来提高收敛速度。

牛顿算法的本质是对数据的二阶统计量进行计算并求逆，从而得到一个更快的迭代方法。此处采用相关矩阵的无偏估计进行计算，但是注意，由于采用平均方法，因此当数据足够多的时候，这种方法得到的结果将对$$R$$的变化影响较小。因此引入遗忘因子，将之前的数据指数加权遗忘，对于非平稳问题可能效果更好一些。

和牛顿法一样是采用了递推估计逆矩阵的方法，采用指数加权的方法更像是RLS算法。

可以证明，这种方法的收敛速度是与特征值扩展不相关的。其实如果收敛因子的取值特殊，则该算法与RLS算法在数学上是等价的。

`RLS算法的本质应该就是后验误差与指数加权`



该算法首先计算先验误差，然后根据上一次估计的逆矩阵对本次迭代的逆矩阵进行估计。估计了本次的矩阵，就可以更新滤波器权重了。

牛顿法改进的一大创新点就是对相关矩阵**求逆**的递推方法。在逆矩阵的递推估计中，存在一个因子$$\alpha$$对过去信号与当前信号的取舍，一般是$$0\sim 0.1$$之间取值。

---

老师为啥讲了一个**牛顿定理的实质**

牛顿方法实质就是`KLT`的最陡下降法。这么一说常见的算法实际上就只有**梯度下降**法呗？

我们首先考虑一个最陡下降算法，


$$
\boldsymbol w(k+1)=\boldsymbol w(k)-\mu \boldsymbol g_w(k)
$$


假如这里的收敛因子用$$\mu\boldsymbol R^{-1}$$代替，就得到了牛顿算法。

而什么是`KLT`呢，简单说就是特征值分解那种变换，将特征值对应的特征向量进行顺序排布、归一化处理，就得到了变换矩阵$$Q$$。这时候，新的参数均在变换域下，原来的相关矩阵就只有对角阵了。

正是由于变换域中的自相关阵为对角阵，因此我们认为输入信号已经被一组正交变换去除了相关性，去除了冗余信息。这就是这个变换的意义。我们得到的这个对角阵正好对应了每一个信号的功率。如果再借助一个归一化变换，我们就能将所有分量的功率归一化。归一化的滤波器系数与输入信号相乘与变换前的结果相同，即正交变换有内积不变性，自适应滤波器因此可以在变换域中进行。将变换域中的牛顿法进行反变换，得到的是最陡下降。

如果我们用`LMS`算法替代最陡下降，用常用的其他变换代替`KLT`，我们将会得到另一种常用的算法`TDAF`或`TDLMS`。

实际上`LMS`牛顿算法是企图对逆矩阵采用`KLT`估计。

---


### 2. 归一化LMS算法

`采用可变收敛因子使瞬时误差最小，可以缩短收敛时间但是增加了失调量`

同样为了提高收敛速度，改进是采用了可变的收敛因子$$\mu$$。改进的收敛因子可以随迭代次数进行更新，但是目的都是加速收敛。

例如我们尽可能减小瞬时平方误差$$e^2(k)$$，最小化方法就是求导并令导数为$$0$$，这样就得到


$$
\mu_k=\frac{1}{2\boldsymbol x^T(k)\boldsymbol x(k)}
$$


这样就能将瞬时平方误差最小化，这样`LMS`更新方程也会一起变更。

一般而言在更新方程中为了控制失调量，常会选择采用一个固定的收敛因子，因为可变的收敛因子是根据瞬时平方误差得到的。为了防止$$x$$取值较小的时候出现较大的更新步长，因此采用一个很小的常数$$\gamma$$附加在分母上，似乎有个拉普拉斯平滑与这个操作类似。

同样的，先计算先验误差，然后进行参数更新。为了保证算法稳定，固定的收敛因子一般在$$2$$以内，一般取值都小于$$1$$

### 3. 变换域LMS算法

`降低特征值扩展，加速收敛`

仍然是对于相关性较强的信号。这种思路是，既然在当前条件下收敛较慢，我们就在一个变换域中进行运算，收敛与变换前是等价的，然后根据变换域的结果进行反变换。这个变换域的要求是，收敛要比变换前快。

算法的系统设计就是在信号输入前加一个变换器，输出结果反变换就能得到结果了。常用的变换是正交变换或说是酉变换。可以说其实相当于一种数据预处理算法。


$$
\boldsymbol s(k)=\boldsymbol T \boldsymbol x(k)
$$


举一个简单例子，就是将特殊位置椭圆经旋转和功率归一化得到圆。原本计算较为复杂的方程就会变得较为简单，收敛速度也会提升。

这时候相关矩阵可以描述为


$$
\boldsymbol R_s=\boldsymbol T \boldsymbol R \boldsymbol T^T
$$


对于满秩的矩阵而言，我们将得到对角化的矩阵，变换矩阵是特征值向量集合，这种标准正交变换被称为`KLT`。这种变换目前是最优变换。

`KL`变换实际上相当于，对信号进行特征向量提取后，采用归一化特征向量组成变换矩阵使其对角化的变换。实际上由于需要进行特征值分解，因此计算复杂度会高一些。常采用离散余弦变换、离散傅里叶变换等存在快速算法又接近`KLT`的酉变换进行变换。

**变换域算法和归一化是结合的！**

### 4. 仿射投影算法

`数据重用算法，在相关性强的时候收敛速度加快`

这是一种数据复用的算法，可以重用前面的数据。这个算法增大了失调量，引入了可调收敛因子以求失调量与收敛速度的平衡。原本的算法是将一次信号输入，这个算法在使用过这一组数据之后，将该组数据保存下来，下一次还参与迭代，保存上限是`L`次数据。

显然这个时候，$$y,d,e$$等在此处都要写作向量的形式。

仿射投影算法的得名正是目标函数，使系数更新时相邻两次的误差最小化，并且增加约束条件使**后验误差**为$$0$$。采用的最小化代价函数为


$$
\frac 1 2 \mid\mid \boldsymbol w(k+1)-\boldsymbol w(k)\mid\mid^2
$$


这个方法被称为是最小距离原理。

采用拉格朗日乘数法将有约束条件的最优化问题转化为无约束最优化问题，代价函数就是


$$
F[\boldsymbol(k+1)]=\frac 1 2 \mid\mid \boldsymbol w(k+1)-\boldsymbol w(k)\mid\mid^2 +\lambda^T_{ap}(k)\left[d_{ap}(k)-X^T_{ap}(k)w(k+1)\right]
$$

计算该代价函数的梯度并将其置零可以推得拉格朗日乘数法算子的解，以及递推方程。参数更新方程中虽然没有出现后验误差，但是却有着后验误差最小化的思想。如果我们将更新后的参数定义为一个超平面，我们可以发现在这个超平面上后验误差均为$$0$$（毕竟这个超平面的定义就是这样的）。

为啥这个方法的名字叫仿射投影呢？我们用后验误差为$$0$$建立超平面。可以发现后验误差为$$0$$的方程正好对应了一个超平面方程，方程的参数就是系数向量，这个超平面上处处后验误差为$$0$$。其他的内容我写在后面的部分了。确实应该理解为**仿射投影**，尤其是投影。

### 5. 量化误差算法

利用短字长或`2`的幂次，降低计算复杂度。思想有点类似于傅里叶变换。

符号误差算法是利用误差信号的符号进行LMS迭代的算法，最小化目标是误差信号模值的两倍。

符号数据算法是利用数据信号的符号进行LMS迭代的算法，这种方法会导致收敛变慢而且可能发散。

## RLS算法

全称是`Recursive Least Square`算法，递归最小二乘。

递归最小二乘法最小化的目标函数是确定的，即在当前系数下要之前时间的所有误差的（加权）平方和最小。即使输入信号相关矩阵的特征值扩展比较大，`RLS`算法也能较好地实现快速收敛。

这个算法通过增加计算复杂度与降低稳定性实现快速收敛。

用官方一些的话讲，就是通过选择合适的系数令观测器件输出信号在最小二乘移一下尽可能与期望信号匹配。最小化过程中始终会用到可得到的所有输入，这一点像极了强化学习问题。话说明白了感觉，所有高级算法不过就是基于简单算法的，到今天我真是体会到这一点。

这里的加权采用的是指数加权，硬件实现容易，递推方便。计算微分得到表达式这个过程极其简单我居然一次性就做出来了。由于这里会用到逆矩阵，因此递推过程中为了降低运算难度也是采用了矩阵求逆引理进行简化，一般来讲`RLS`更新存在两种方式。

- 一种方法是，估计了逆矩阵之后，再估计互相关，然后直接相乘得到结果
- 另一种是，估计了逆矩阵后，利用误差递推$$w$$系数的更新

不过由于求逆引理本身的问题，求逆过程中存在稳定性问题。

这里介绍一个正规方程，即假如我们已知了方程


$$
\boldsymbol X^T(k)\boldsymbol w(k)=\boldsymbol d(k)
$$


解方程的时候，由于左侧是矩阵与向量乘积，但是不是方阵，因此一般不好进行逆矩阵移项。这时我们往往采用


$$
\boldsymbol X(k)\boldsymbol X^T(k)\boldsymbol w(k)=\boldsymbol X(k)\boldsymbol d(k)
$$


woc然后就牛逼了，这样操作之后就能移项回去了，这样就能得到方程的解，我们可以管这个方程叫**正规方程**

这样移项一下我们还能看出来$$\boldsymbol X(k)(\boldsymbol y(k)-\boldsymbol d(k))=0$$

这就是`RLS`的**正交性原理**

如果我们的信号是一个有遍历性的平稳随机过程，则最小二乘解趋于维纳解；如果迭代次数较多，我们对于$$R$$的估计值将对变化不敏感，因为加权过程中利用到了所有信号，一个信号难以改变前面已经存在的较多信号。另外相关矩阵是时变的时候，我们的估计结果肯定是有偏的。

另外我们对自相关矩阵进行确定性初始化也可能导致出现偏差。如果加权因子是一个小于 $$1$$的数，当点数足够多的时候偏差趋近于$$0$$。

`稳态特性`

最小二乘算法的估计是无偏估计，假设我们计算某加性测量噪声，对系数向量进行期望运算，可以得到系数是一个无偏估计。这里需要说的几个东西是

- 逆矩阵依赖于所有输入数据，因此在输入较多的时候（即$$k$$较大的时候），其变化是越来越小的，逐渐趋于对$$\boldsymbol R^{-1}$$的稳定估计。
- 根据正交性原理，误差向量独立于输入向量，因此即使是不准确的估计，这两个量也是较为正交的。计算的时候，在误差要求不大的时候，可以直接计$$0$$
- 分析$$\mathbb E[\Delta w]$$的表达式，我们可以看出随着$$k$$增加，这个值越来越小
- 我们可以得到的结论是
    - 最小二乘算法是系统辨识问题的最优线性无偏解
    - 如果加性噪声是白噪声，那么最小二乘解达到了克拉美劳下界，得到最小方差无偏解。

## 自适应格型RLS算法

这一部分，我摊牌吧，我就不会。

说了是格型算法，是因为其物理实现是格型结构，按模块实现。至于模块化实现如何降低运算量我是没看出来。推导之前我们可能还不知道为什么是格型，推导以后比较容易看出来。

简单介绍，这种算法是在最小二乘意义下实现**前向预测**与**后向预测**，实现**阶数**与滤波器**系数**均可以自适应调整的算法。

### Forward

前向预测，就是用前面的输入数据对当前数据$$x(k)$$进行预测。首先对于当前输出，我们写作


$$
y_f(k,i+1)=\boldsymbol w^T_f(k,i+1)\boldsymbol x(k-1,i+1)
$$


预测器的前向运算就是这样的，这时候我们是采用$$i$$阶滤波器，有$$i+1$$个抽头。不过这个只是一组，对于最小二乘方法来说，我们往往是用上所有数据的。此时虽然单步误差为


$$
\varepsilon_f(k,i+1)=x(k)-\boldsymbol w_f^T(k,i+1)\boldsymbol x(k-1,i+1)
$$


计算所有的误差组成向量就是


$$
\boldsymbol \varepsilon_f(k,i+1)=\boldsymbol x(k) - \boldsymbol X^T(k-1,i+1)\boldsymbol w_f(k,i+1)
$$


这里的$$\boldsymbol x$$和$$\boldsymbol X^T$$实际上是加权的，因为最小二乘的时候为了有一定的抗非平稳的性能，采用了指数加权的方法，这里就直接计算到向量中了。由于是误差平方和的加权，因此这里的加权系数的指数取到一半就可以了。

这个表达式其实可以写成矩阵乘法。有没有必要另说，但这样写会给人一种牛逼的感觉。


$$
\varepsilon_f(k,i+1)=\boldsymbol X^T(k,i+2)\left[\begin{matrix} 1\\-\boldsymbol w_f(k,i+1) \end{matrix}\right]
$$


实际计算的时候要计算误差平方并最小化，此时计算误差平方并求导，找出导数为$$0$$的点，我们认为这里就是最优系数向量的取值。我们姑且将结果写成


$$
\boldsymbol w^T_f(k,i+1)=\boldsymbol R_{Df}^{-1}=(k-1,i+1)\boldsymbol p_{Df}(k,i+1)
$$


这里的相关矩阵我们认为就是确定性矩阵了，而后面的互相关向量也就认为是确定的了。带入这个结果我们得到最小的误差$$\xi_{f,min}^d(k,i+1)$$，将上述两个表达式写在一个矩阵中就能得到


$$
\left[\begin{matrix} \sigma^2_f(k) &\boldsymbol p_{Df}^T(k,i+1)\\\boldsymbol p_{Df}(k,i+1)&\boldsymbol R_{Df}(k-1,i+1) \end{matrix}\right]\left[\begin{matrix} 1\\-\boldsymbol w_f(k,i+1) \end{matrix}\right]=\left[\begin{matrix} \xi_{f,min}^d(k,i+1)\\ \boldsymbol 0 \end{matrix}\right]
$$


其实按照这种分块方式，我们可以推导得到


$$
\boldsymbol R_D(k,i+2)\left[\begin{matrix} 1\\-\boldsymbol w_f(k,i+1) \end{matrix}\right]=\left[\begin{matrix} \xi_{f,min}^d(k,i+1)\\ \boldsymbol 0 \end{matrix}\right]
$$


这个结果其实表示了，我们可以通过前向预测，在最小化最小二乘误差的条件下获取滤波器系数，还能根据这个进行阶数的预测。好神奇呢。

### Backward

草根本不会写个P。





## 复习系列

### 第一章

- 例题1.  信号增强，其实和我实验报告写的是一样的。这个框图是完全可以运行的。

其实考试的重点并不在这个例题，而是在于这些概念。

- 自适应系统主要由三个部分组成
    - 结构，我们基本上只学了FIR实现
    - 应用，可以说有四种，上面详细讲了。
    - 算法，自适应算法是实现自适应滤波的基础。
- 自适应算法主要有三个内容
    - 最小化算法。寻找一种最小化目标函数的算法，常见的有牛顿法，改进的牛顿法以及梯度下降法。
    - 代价函数（目标函数）。非负性、最优性。常用代价函数有MSE，瞬时、加权平均、最小二乘（就是平均平方和）等
    - 误差信号。常见误差信号就是目标减去实际，但是实际设计中可以采用不同的方式。

### 第二章

- 例题1.  自相关函数计算。注意AR信号的自相关计算。以及需要积分的运算过程。
- 例题2.  一阶自适应滤波器。我他妈差点问老师，这题啥都没说为啥系数就取两个呢？人家**一阶滤波器**五个大字我还圈起来了。这个题就是得理解AR过程自相关函数。
- 例题3.  简单的我都不想说。最后这一个参数更新的部分很可能考。留意一下。
- 例题4.  
    - a.  对于信号增强这一问，需要掌握AR信号的互相关，二阶矩阵的快速逆。
    - b.  信号预测。
    - 算了都不想说了自己去做吧都挺简单的



作业也做做吧

- 计算相关矩阵实在是太困难了，希望老师不要故意搞我们。
- 注意阶数
- 注意矩阵乘法的映射关系
- 后面五个题看了一遍就没有我会的。

### 第三章

LMS算法的精髓在于，使用瞬时值来估计期望值。由于采用瞬时值时结果与期望值存在偏差，但是在统计平均的意义下，瞬时值收敛于期望，因此也可以用来进行运算。实际运算结果也是符合推导的。

- 复数算法就是将转置换成共轭转置的方法。不转置的地方就直接用共轭。
- 例题1.  信道均衡问题。这里说明依据，**输入信号**永远都是说**自适应滤波器**的输入信号。自适应滤波器的输出信号就是$$y(k)$$。计算离散系统的自相关互相关还是比较复杂的，在积分的时候记得共轭、延迟的表示方法，留数法计算积分。这里有一个最优延迟确定的问题，此处采用的方法是对L进行代数，然后计算自相关发现在L=2的时候就已经不能产生较强的自相关了，所以采用相关性较强的L=1。这里的思路是，当延迟一个单位的时候，自相关性最强，我们可以认为这个时候对信号进行均衡效果最好。也许进行预测的时候也是一样的。
- 例题2.  就是简单的自适应滤波问题。这里说到周期信号的相关函数求法。两者的周期相同，则计算周期内的平均值作为实际值即可。周期内取点可以参考标准形式下$$2\pi$$的分母。这里一般需要计算器。
- 例题3.  分析周期信号输入时LMS算法性能。直接分析十分复杂，这里还是用周期分析方法比较好。
- 例题4.  为了得到最小化的目标，一般需要找到梯度，然后根据梯度计算积分。要知道标准式的形式！是减去收敛因子乘梯度。记得最简单的梯度表达式的形式，是P26的结果。
- 例题5.  这个题我拿去仿真过了。这个题拿来考试是不实际的，基本上算不出来。结论是特征值扩展越大越难以收敛，但是最终不会影响最后的MSE。LMS算法的失调和超量MSE公式可以记一下。
- 后面几个例题我没有看。实在是有点难度

作业稍微写写

- 预测器这个题，正常流程做吧，根据相关取的话，余弦信号不如取一个单位的延迟
- AR信号输入求维纳解这种题都做爆了，回头有机会仔细推导一下好了
- 数据重用那个题考过一次了，如果让计算收敛，就用$$\Delta$$进行计算，结果还是一样的结论，毕竟指数怎么取，想要收敛范围都是一样的；如果问最小化目标函数，一般就是通过梯度去找，去慢慢凑。复杂度，LMS改进算法得想办法说出来他比原来的LMS好。
- 一种常见的z传递函数。冲激响应。不知道为什么，课本上的收敛因子上界都选了$$\frac{1}{(N+3)\lambda_{max}}$$，当然也不排除是迹加两倍最大特征值的倒数。
- 3.13这个题，就按照P66-67的做法做就可以了，一个滞后滤波器和一个一阶AR滤波器串联就行。

### 第四章

- 例题1.  这里说的方法上课似乎没有讲，那么至此为止我就写了前半部分，即计算相关矩阵。还是多BB一句，相关矩阵就先计算相关函数，然后往里面填空就完事了。
- 例题2.  变换域LMS算法，这里需要知道，二阶情况下的**KL**变换问题。这里采用了标准余弦变换，这个矩阵要记得，在变换域中计算结果要会算，画图部分不太容易考，反正记得它是可以加速收敛（尤其是对于特征值扩展较大的信号，这里可以先变换为特征值扩展稍小一点的信号）。
- 例题3.  这又是一个没啥价值的题。给出一个代价函数，推导更新方程。代价函数的导数就是更新方向，我们记得标准更新方程中，新系数是旧系数减去梯度（乘一个收敛因子），此处要会求导。实话讲，这个形式实在是太复杂了，四阶情况换成二阶相乘，还要进行特别的代换。**如果有机会我再回来看看吧**
- 例题4.  归一化方法就是采用可变的收敛因子，是选择合适的收敛因子使瞬时误差最小。如果担心分母为0，可以采用一个小系数$$\gamma$$来修正。但是这个题的问点不在这里，他让我们先写俩超平面的方程，然后联立求解就是滤波器系数。这个对于满秩的情况来说是比较好的，有唯一解。注意这里写平面方程，可以用字母代替向量。超平面可以是线面体等线性方程的推广形式。
- 例题5.  复数归一化。考到算倒霉吧，反正也没讲复数这部分的。这题就是在放屁，不看了。
- 例题6.  写出表达式，把能带进去的都带进去就行了。不太理解第二问为啥要这样写，反正我是推导的时候不是这样写的，仔细看了一下其实过程差不多。

作业看看

- 第四章作业都是推导和仿真，老师出这部分题的可能性看起来不是很大。
- 建立一种对应关系，误差系数就是$$\Delta w^Tx$$。第一题的推导过程实在是有一些脑洞，不背板子的话应该是做不出来了。
- 推导后验误差为0的LMS牛顿其实就是推导RLS算法吧？woc我居然推出来了，我好牛逼啊。
- 导出更新方程，直接推不出来就求梯度。仿射投影是拉普拉斯乘数法约束条件下的目标函数。仿射投影下$$X_{ap}(k)$$中是列向量。
- woc他给的都是些什么玩意。

突然想总结一下，LMS问题对于已知代价函数计算递推方程的时候，直接求导就可以了，带入到原始的迭代方程中即可；对于已知递推方程求解最小化代价函数的问题，需要手动积分。我寻思没学过矩阵分析的人怎么可能分析的出来，实在不行就记住几组导数得了。这些题最后还得过一遍。

### 第五章

- 例题1.  理解一下什么是最小二乘解。RLS算法的最优解仍然是$$w=R^{-1}p$$，但是不同的是，这里的相关矩阵和另一个向量，都是通过当前所有数据的加权求和得到的。实际上推导RLS算法采用的是后验误差，毕竟RLS本身定义代价函数就是用后验误差定义的。如果按照加权的形式写好，那么其实最小二乘解有点像正规方程解（其实就是吧！）。
- wdnmd后面的题基本上都是仿真

### 其他所有

- 这个第六章完全一个字都没讲，第七章看着办吧
- 我觉得闭卷考试考第七章就是故意难为我们



### 这里把忘记的东西再bb一遍

1. 自适应系统三要素。
    - 应用
    - 结构
    - 算法

2. 自适应算法三要素。
    - 最小化算法
    - 目标函数
    - 误差信号

3. FIR滤波器的传递函数是全零点的。

4. 四个基本框图的绘制和原理。注意四种应用下，输入信号都是滤波器的输入，而不是总输入。

5. 信号增强应用中，输出误差信号就是我们的期望结果。

6. 向量内积就是“**当前时刻**”的卷积和。

7. N阶AR过程就是N阶马尔科夫过程。马尔科夫过程的特点是无后效性，所有状态每一个时刻的状态都保留了历史所有状态信息。

8. Z域积分，记得**共轭**，积分的时候记得$$dz/z$$，还有留数法。

9. 相关矩阵的若干特点。
    - 半正定
    - 厄米特矩阵
    - Toeplitz
    - 特征值与特征向量
        - 厄米特矩阵特征值为实数，半正定特征值大于等于0。特征值之和等于相关矩阵的迹。

10. 白噪声的自相关函数就是单位冲激。对于白噪声乘一个信号的情况，求自相关之后

11. 积化和差公式，只要乘积里面有sin函数，那么和中就必然全是sin，且sin在前的时候为和、sin在后的时候为差；如果是两个sin，结果就是两个cos，且前面是负号；如果没有sin，那结果里面也没有sin。 

12. 记住AR信号的自相关推导过程。自平方那一步不能忘了。

13. 实际上一阶AR信号的相关矩阵，求逆之后系数正好就没有了，而且原来如果是负的，还会变成正的。

14. **对于复随机过程，要计算共轭。**

15. 维纳滤波器，只要是使用了MSE做目标函数的都是维纳滤波器，FIR维纳滤波器存在一个最优解，$$Rw=p$$，这个对于其他条件下的变体也是存在类似结论的。

16. 维纳滤波器的推导中有一些结论是可以记住的，比如矩阵的求导。

17. 其实在维纳解这边我们就已经推导出来过正交原理的问题，即误差与输入是正交的。

18. 考虑到线性滤波器的原因，实际上误差信号与输出信号也是正交的。相比于上面的与输入向量正交，这里是两个数值乘积的期望为0。

19. 提醒**二阶矩阵的快速求逆公式**！主对角线俩是互换位置的，副对角线俩只是求负数。

20. 顺便联动一下**AR信号互相关**。谁括号里的值大，上面的分子就是谁的极点，下面是两个极点的乘积。

21. MSE曲面这里，很无聊的事情。相关矩阵本来特征值未必相等，不同的特征值与不同的变量相乘，构成了不同轴上的取值，这时候就是个超椭圆。对此时的误差函数求导得到的就是特征值对应的导数值，特征值越大的梯度就越大，下降速度越快，这时候为了稳定这部分下降速度，就会以这一部分为基准选择收敛因子。说到底，变换域中只是将原有的图形经过线性变换后得到一个圆，这时候梯度就都一样了，收敛也会变快很多。

22. 牛顿算法其实就是变换域中的梯度法。

23. 从期望的角度来看，收敛因子取值应该是$$\frac{1}{\lambda_{max}}$$以下的。具体推导主要是根据梯度法的更新方程来的，主要采用$$\Delta$$的方式进行推导的。由于第一次推导是在第二章，这时候还没讲过LMS，因此更新方程还是经典的采用梯度的方法。推了一遍发现忘了很多细节。

24. 我完全一眼都没看时间常数这一部分的内容。

25. 后面图中绘制的，梯度下降法按照梯度方向，而牛顿法从直线方向很快达到了收敛。

26. 应用回顾这里尤其重要，四种应用的详解就在这里。
    - 对于系统辨识问题，未知系统采用了无限长响应的方式（也更加接近实际）。对于某一时刻的输出，目标系统一定是无限长响应与无限长输入之间的卷积。但是我们的N阶系统却只有N+1个系数。计算出来的MSE最小化之后，我们的系统能够较为完美地拟合前N+1个系数，但是对于后面无限长的结果无能为力。
    - 增强的时候输出的误差信号才是我们需要的信号。两路噪声是存在一定相关性的，否则不容易计算。正如课本所言，增强方法的有效性取决于两路信号的相关性，另外这里还说了一句很重要的话，**一般在滤波器结构中某一路的延迟都是为了能够获得两路信号最好的互相关。**
    - 信号预测问题，就是用过去信号预测现在的信号。取延迟也是为了相关性吧。这里想算一下周期函数的平均相关。算了一遍。此处由于是采取了饱和的平均处理，因此此处k的取值不会影响最终结果。我们一个一个的计算平均值后就得到了相关值。（例题3.3）
    - 信道均衡。其中的输入信号就是滤波器的输入，与其他的没啥关系。信道就相当于一个$$z$$传递函数，延迟$$L$$是要求保证两路相关性最强。这里用相关性最强，是因为有利于滤波器的优化。
    - 总结起来，有延迟的就是考虑到了互相关最大保证易于收敛嘛

27. 第二章的例题已经做了好几遍了，这里也不想再推一遍了。看准了阶数做吧，应该是没错的。几种滤波器采用什么输入什么输出，规则是什么样子的，这些都记清楚了。习题都写在上面了。

28. 有时间回来把作业里的相关矩阵重新算算，但是我觉得有的应该是算不出来的。

29. 第二章最后一面作业很难，基本上不是没学就是做不出来。明天有时间推一下复数算法，今天是没时间了。

30. 最小均方这边主要思想就是用瞬时代替均值，瞬时值在统计平均下是向期望（维纳解）无偏收敛的，且计算复杂度低，有限精度实现稳定。

31. 进入LMS区之后，就可以随便用瞬时值结果了。例如$$R(k)$$这样的。不过考虑到是估计值，可以加一个hat

32. **最优误差**，其实就是测量噪声。测量噪声是加在期望信号上的。

33. 通过对一阶统计量的分析，作者推导了上面说的那个收敛因子的范围，但是通过对二阶统计量的分析，作者导出了另一个稳定性条件，即$$1<\mu<\frac{1}{tr[R]}$$

34. 作业里面的收敛因子都是采用了后面的另一个结果，$$\frac{1}{(N+3)\lambda_{max}}$$，至于为什么这样选，我问过老师了，老师没有回复。

35. LMS算法下的超量MSE公式比较好记，实际上自己推导也不是不行。失调就相当于比掉了一个测量噪声的平方。

36. 复习的时候我一直没有看最低收敛次数的计算。最优收敛次数计算为

    $$k\sim 4.6\frac{(N+3)}{2}C$$

    其中$$C$$为特征值扩展

37. 滞后向量元素的那个题就很真实。做法更新在上面了。

38. 不平稳条件下的超量MSE就是原来的MSE加上一个滞后引起的超量MSE。

    $$\frac{\sigma_w^2}{4\mu}\sum\limits_{i=0}^{N}\frac{1}{1-\mu\lambda_i}$$

39. 该计算共轭的地方计算共轭，就是复数算法。

40. 留数法计算的是单位圆内的极点。

41. 第三章的例题里面首次提到了周期信号计算相关矩阵应该采用周期平均方法。此时由于k取值没有啥影响所以计算的时候可以不管它。

42. 后面例题是经典的让我们通过更新方程计算最小化目标函数和收敛因子范围的题目。

    - 收敛因子，写成只有两者的递推形式。令滤波器系数收敛的时候整理的式子有可能并不是只有二者，但是这时候可以大胆地只考虑两者。
    - 最小化目标函数，写成标准式。

    

43. 有限精度实现的时候，收敛速度可能减慢，并且造成更大的超量误差，

44. 线性约束和快速自适应仿真我真的一眼都没看

45. 计算平均相关的时候两边都要`-m`

46. 数据重用真的不想看了。

47. 反正就是那么个原理嘛，计算$$\mu$$的时候写成递推形式，计算目标函数的时候往里凑，这里有一个链式求导过程而已。他这里认为一次迭代不是内部迭代而是外部迭代一次，那显然是数据复用快，但是计算复杂度一定是不知道高到哪去了。

48. $$\mu_{opt}=\sqrt{\frac{(N+1)\sigma_w^2}{4\sigma_n^2tr[R]}}$$

49. 最优收敛因子是对均方误差求导为零得到的，但是这个数未必在稳定范围内

50. 没看符号误差和符号数据算法，这俩应该是没讲反正。

51. 推一下LMS牛顿？LMS算法本来是基于最陡下降来的，但是实际上也可以套牛顿法。

52. 但是牛顿的丢人之处在于计算复杂度实在是太高了，所以又给加了个降低求逆复杂度的方法。

53. 由于LMS方法又用了加权的方法进行估计，应对非平稳情况，就和RLS算法有点相似了。实际上当这个加权系数取值合适的时候就是RLS了。

54. 归一化，龟龟归一化，完全看不出来哪里归一化了。归一化算法的目的是让每一次迭代中的瞬时误差都向着降低的方向进行。为此采用了可变系数的方法。分母往往使用一个小系数防止为零。上面那个系数应该取2以下，实际中往往取1以下。

55. 变换域这边题目常常是让用离散余弦变换。离散余弦变换是一个近似 klt的变换，而且不随矩阵变化。而且是酉变换。欧氏空间内的酉变换就是正交变换，是一种不变内积 变换。

56. 今天再看一遍仿射投影。。

57. 仿射投影说是一种数据复用算法，但是从几何角度理解就比较有趣。首先数据复用算法是不用加权的，与RLS算法不同。另外，如果写成超平面的形式，就会有

    $$d(k)-w^T(k+1)x(k)=0$$

    这种超平面出现。不知道什么是超平面我也不想解释了，总之这里有一些不同。传统的超平面

    $$\boldsymbol a^T\boldsymbol x=0$$

    这种超平面，一般认为$$x$$是变量，$$a$$是参数，一组参数决定一个超平面，满足条件的取值$$x$$都在超平面上。

    但是这里的超平面不一样，这里是每一个$$x$$决定一个超平面，满足上式的$$w$$都在超平面上，这样超平面上都是指定输入$$x$$的解$$w$$。

    如果我们采用的输入$$x$$够多，则每次迭代都会从上次计算的结果垂直搜索下一个超平面上对应的解。经过足够多次迭代，就会收敛在最优解附近位置。

    加上收敛因子的时候，我们会认为一次搜索得不到最优解（不能收敛到超平面上）

58. 又到了我最喜欢的例题时间。随机梯度算法是这个，似乎上下两个是不同的求导方法。第十四章给出了定义，但是我觉得如果老师考这个老师就是铁憨憨

59. 有了上面的分析，这里就不难理解了。俩系数的超平面其实是一个直线，直线交集就是一个点，解出来的系数就是这两个直线的交点；

60. 复数相关的题我真的不想看了。

61. 离散余弦变换有一个特点就是变换结果集中在左上角，其余部分较为稀疏。我试过了，是真的。变换域算法是考虑**旋转和功率归一化**的。旋转主要是离散余弦变换完成，**归一化就是前面的归一化算法了**，这一点我居然才看见。当然也不完全相同，归一化算法是希望每一步优化的时候误差都会减小，这里没有这个要求，这里的归一化是要让变换结果前面乘一个归一化矩阵，让每一个输入信号的变换结果的功率估计归一化。

62. 记住变换域方法的更新公式8

63. 这一次没有仔细看符号误差方法。符号误差方法中有一个比较重要的推导是

    $$E[sgn[e(k)]x(k)]=\sqrt{\frac{2}{\pi\xi(k)}}E[e(k)x(k)]$$

64. 这几个作业题真的弟弟。

65. RLS区到了@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

66. 递归最小二乘，就是迭代叠叠乐。又要把逆矩阵拉出来说事了。会利用到所有的历史数据。

67. 最小二乘解。就是把所有的输入数据和加权系数乘起来写成矩阵，另一边是期望的结果。没有意外的话，方程数等于未知数个数的时候，就可以求出唯一解，欠定或超定时往往不存在唯一解。RLS问题实际上解决的是超定问题。但是书上给的例子是关于最小二乘解的，最小二乘解就是恰好有唯一解的问题。

    $$X^Tw(k)-d(k)=0$$

    $$XX^Tw(k)=Xd(k)$$

    这样就能求解$$w$$，这被称为正规方程法。

68. 求正规解的题，有时候会觉得矩阵表示有问题。那一定是你算错了。

