---
layout: post
comments: true
title:  "水区"
excerpt: "-"
date:   2018-10-20 12:42:24
categories: Notes
---

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

没有意义的内容全部集中在这里了，不定期更新，日期后注。

---

## 1. ユメセカイ

[ユメセカイ - 戸松遥](http://music.163.com/song?id=417613546&userid=417359311)

第一部中桐老爷在医院醒来后的BGM。

> 想说一句，谷歌和百度的翻译很灵性。我不是很懂日语规则，但是谷歌翻译出来的画风是“Yumesekai”，而百度翻译出来就是“梦世界”。两家的NLP模型看来对语言的处理确实存在一定不同，对于片假名的解读，谷歌倾向于表达注音这个意思，而百度却指出了这个注音可能的含义。如果出现多义性问题，百度的表现就不得而知了w。PS：后来咨询得到，歌名与欧美词汇在日语中倾向于写成注音的形式。因此谷歌的翻译结果在这一项上效果可能更优。
>
> 官方翻译应该是“梦想世界”，这段BGM出现的位置，原本标志着川原砾当年对“刀剑神域”这个世界观的描写的结束，但在后来这个故事还是继续延续下去了。从我个人角度来看，SAO的故事如果在这里结束，也是很好的了...后面对这个故事的不断增补，反而给人一种狗尾续貂的感觉。每当听到这个BGM，总会让我想到高三的时候，每天十点十分就会熄灯，而楼管大爷也会在十一点前不断地查房，检查宿舍纪律。楼道中的微弱灯光透过门上的玻璃照到我的床边，光斑大小刚好够放下一本“刀剑神域”。高三下学期最后的时候，很长一段时间，在十一点后我都会从枕头下面拿出这本书逐字逐句地仔细阅读，也算是我高中时期最辛苦的时段中唯一的乐趣了。虽然在看小说的时候从来没有看过动画作品，可是就算是第一次听到这个BGM我也自然地联想到了这个片段，可以说是非常用心的作品了。
>
> 从现实的角度来讲，我可能无论从哪个角度来看都是一个不合格的人。每当这样想的时候，都会想再来听几遍这样的歌曲，好让我尽快抛弃这样的想法。
>
> 2018年10月20日12:42:08

## 2. 微机原理课程设计

**这一段比较长**

TPU是特化的专用处理器。

- 通用处理器

  通用处理器一般指我们目前使用的CPU。相对于专用处理器，其性能没有针对某些特殊任务做特别的优化，因此在各项使用中表现都很正常，没有特别出色的地方。

  - **复杂指令集**（Complex Instruction Set Computing;**CISC**）通用处理器

    目前正在学习的x86（例如8086）就是基于复杂指令集的处理器。复杂指令集是一种微处理器指令集架构，每个指令执行若干低级操作（存储器中读取、存储，计算等，全部集于单一指令中）。复杂指令集指令数目多而杂，且每条指令字长并不相等，计算机必须加以判断读取，在性能上付出代价。

  - **精简指令集**（Reduced Instruction Set Computing;**RISC**）通用处理器

    对指令数目与寻址方式都做了精简，使其实现更容易，指令并行执行程度号，编译器的效率更高。目前而言采用精简指令集的处理器主要是ARM、AVR、MIPS以及IBM的Power Architecture处理器。

    > 题外话，早期计算机行业中编译器技术并不发达，许多程序以机器语言或汇编语言来完成。为了便于程序编写，当年的程序设计者设计出越来越复杂的指令，可以直接对应高级编程语言的高级功能。在那个年代的看法是，硬件设计更加容易一些。
    >
    > 当时的内存还极小，在缺乏内存的条件下，我们需要程序更加精简，毕竟每一字节都很宝贵。因此要对信息做高度编码。当时内存不仅小，而且还慢（磁芯时代），如果我们对信号做高度编码，访问频率可以下降。
    >
    > 上课也讲过寄存器的设计很贵，每多一位就要提高很多成本；而且以当时的设计水平，额外设计更大的寄存器也是难度较高的。
    >
    > 总体来说当时的人们就是又不愿意多花钱，又懒得自己动手，因此编个程序累skr人。微处理器设计师们也尽可能的让每一个指令做更多的工作，这就是复杂指令集。
    >
    > 精简指令集的优化思想主要是
    >
    > 1. 统一指令编码
    > 2. 泛用寄存器
    > 3. 单纯的寻址模式（复杂寻址模式被简单计算指令序列替代）
    > 4. 硬件支持少数数据模式（部分CISC计算机中有处理字节字符串的指令，这在RISC中一般是不会有的）

    明面上看，复杂指令集都是在服务一些性能相对较差的嵌入式设备处理器，大公司中似乎只有IBM在使用复杂指令集。以**Intel**为例，其不使用复杂指令集的原因是考虑到了代码的兼容性问题。实际上Intel在底层设计时也会采用复杂指令集，人家只是不放弃CISC。Intel在进行了多年的尝试（例如Itanium）后最终也发现，RISC和CISC的混合方法可能是最优的。

    > 参考资料1：[Quora: Why didn't Intel move from CISC architecture to RISC architecture?](https://www.quora.com/Why-didnt-Intel-move-from-CISC-architecture-to-RISC-architecture)
    >
    > 参考资料2：[Stack Overflow:Why does Intel hide internal RISC core in their processors?](https://stackoverflow.com/questions/5806589/why-does-intel-hide-internal-risc-core-in-their-processors)

- 专用处理器

  专用处理器常用于特殊任务的高效运算。相比于CPU在图像处理的运算能力上，GPU要强得多。这也是GPU被设计的原因。GPU在浮点运算中的性能远高于CPU，而且对并行计算适应性更高，在PC机中，这就是一块专用处理器。这里不再对专用处理器做展开说明，只对TPU做一点说明。

  21世纪是生命科学的世纪；近年间来看，21世纪似乎又是一个“人工智能”的世纪。人工智能的说法是不妥的，因为我们眼中的人工智能的突破进展，实际上是**处理器性能提升**与**数据量暴增**两个原因共同造成的。语音与图像数据量的增长，以及对海量数据的有效利用促成了目前的计算机视觉以及自然语言处理等行业的发展，其中也离不开深度学习方法的巨大贡献。这里就列举一个实际应用的例子来说明专用处理器的必要性及其性能。

  深度学习领域中的许多运算都是浮点运算，这样的运算让CPU计算起来速度就会慢很多。因此在对神经网络参数做训练时人们会采取GPU进行，其速度会达到CPU的若干倍。但是Google毕竟是Google，不仅在学术上要走在行业的尖端，就连设备也要。为了配合自己发布的深度学习框架TensorFlow，Google还特意设计了一款硬件设备用于深度学习上的运算加速，那就是TPU（**Tensor Processing  Unit**，参考资料[In-Datacenter Performance Analysis of a Tensor Processing Unit](https://arxiv.org/abs/1704.04760)，1704.04760）。文献中主要论证了TPU的可实现性以及其性能指标。

  神经网络中的计算主要是矩阵的乘法、加法与非线性函数。典型的一个运算过程是

  $$y=\sigma(Wx+b)$$

  其中，$$W$$和$$x$$都是矩阵。$$x$$代表运算过程中的变量，常被称为特征(feature)。$$W$$是权值，因此$$Wx$$就是简单的矩阵乘法计算。$$b$$被称为偏置(bias)，整个结构就是一个变量较多的线性函数。然后人们向这个线性运算中添加了一个非线性函数，使结果出现非线性量，$$\sigma$$函数就是一个非线性函数，nn中常见的有Sigmoid，tanh，ReLU，Softmax等。详细内容可以看CS229那篇博客（

  TPU所依赖的基本技术为

  1. 采用低精度（8bit）计算。8个二进制位也就是2个十六进制位。在深度学习的参数计算过程中，经常会对参数进行归一化处理，因为Scale过大或过小的参数可能会促进梯度爆炸与梯度消失，更会导致计算量暴增而降低处理速度。根据上面文献的信息，将数据位数缩减到8bit仍然能够满足运算需求，对深度学习的参数运算影响很小。另外还能显著降低功率（或者说提高 性能/功耗 的效率比值）。
  2. TPU中采用了特殊的运算结构（Systolic Array）。在国内统一翻译为脉动阵列，Systolic是心脏收缩的意思，由于并行数据流经硬件连线接入处理器节点网络，然后数据被组合处理合并或排序并导出成为计算结果，类似人的心脏收缩时的工作流程，因此得名。该阵列往往用于特定操作，例如大规模并行积分、**卷积**、相关或**矩阵**运算。
  3. TPU采用了更大的片上内存，以此来降低对计算机内存的访问
  4. 直接将神经网络中会用到的激活函数算法硬件实现，并为数据运算提供了高级指令。可以将来自TensorFlow的图式运算结构API调用直接转化乘TPU指令。
  5. 相比传统的计算芯片，TPU设计极为简单。

  2016年TPU首次发布，但在这之前就已经在Google的项目中服役，最为出名的项目包括Google街景服务以及DeepMind的围棋软件AlphaGo等。2017年谷歌I/O年会上发布了第二代TPU，2018年五月发布了第三代。初代的TPU在性能上并不能超越同期的GPU，但随着更新迭代，TPU在处理TensorFlow框架下运算的能力越来越强。就在不到两周前，Google发布了一个称为BERT的NLP模型，号称目前最强的NLP模型（确实强=-=）。该模型在训练时会使用到TPU。完全体BERT训练时使用到了64个TPU芯片，总共使用4天时间就完成了训练（这一次训练大约要消耗50000美元）；如果在NVIDIA的Tesla P100处理器上进行训练，恐怕需要8块P100芯片并行训练1年。

1. 详细介绍神经网络的运算

   神经网络的典型计算是

   $$y=\sigma(W·x+b)$$，这个是一个前向传播的过程，可以参考图片

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/Andrew.png)

   每一个神经元上存储的是权重，输入特征x经过特殊的规则与每特定的权重进行乘法运算，可以等效为一个矩阵乘法过程——一个线性过程。增加了偏置后就相当于增加了一个加性值b。经过这样处理后的结果，对其进行$$\sigma$$函数操作，使其可以出现非线性特征。至此神经网络的一个前向传播计算完毕，计算的结果将进行下一层的运算。

2. TensorFlow是什么

   TensorFlow是谷歌开源的一款深度学习框架，实际上是C++底层框架的一个接口。该框架分为Python版和C版，使用起来十分方便，降低搭建深度学习系统的工程量。

3. 脉动阵列（Systolic Array）是如何实现的。Systolic Array实际上不属于SISD，SIMD，MISD，MIMD中的任意一个，而是一种特殊结构。[知乎链接](https://zhuanlan.zhihu.com/p/26522315)给了一个较为清晰的解释，我们可以理解为，将权值矩阵按规则排好顺序，将输入的数据也按顺序排好，然后将权值从上面送入，x从左侧送入，每一个cell进行一次乘法运算，就可以在几个时序后完成整个矩阵的运算。

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/systolicarrayI.jpg)

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/systolicarrayII.jpg)

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/systolicarrayIII.jpg)

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/systolicarrayIV.jpg)

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/systolicarrayV.jpg)



2018年10月21日16:42:08

## 3. 刮墙

温暖的话语总是能够激励人心。

2018年10月22日10:21:20

## 4. 日常

GitHub是炸了吗，怎么push不上去的。

GitHub炸掉的这一段时间里面，发生了很多事情。

tmd都发生完了怎么还没恢复

2018年10月22日22:59:43

今天帮非电子专业的同学简单讲了一下DSP的相关内容，突然发现自己会的好多= =、哪怕只是课内的知识，原来在不经意间已经累积了很多了。

2018年10月24日13:44:40

## 5. 位扩展与字扩展

生产的存储芯片的容量有限，但是随着应用需求的变化，人们对存储芯片的容量有了更高的要求。人们常常将多块相同的存储芯片组合起来形成一个存储容量更大的存储器，组合方式主要是位扩展和字扩展。

1. 位扩展：将数片位数较少的存储芯片组合成位数更多的存储器。就像并联一样。位扩展示意图如下

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/bit_extend.png)

   四块32K 8bit的存储芯片组合为一块32K 32bit的存储器。新的存储器仍然有32K个存储单元，但是每个单元有32bit。对于32K存储器，需要15条地址线，因此地址线从$$A_0$$到$$A_{14}$$。地址线同样的连接到四块芯片上，与此同时还有两条控制线，一个是片选芯片，输入此时的片选信号，如图的$$\overline{CE}$$就是片选信号。$$\overline{OE}$$是另一个控制信号，用于表示此时操作为**读**还是**写**。

   每个存储器都按照存储单元的位数输出8bit的数据线（例如$$D_0$$到$$D_7$$）。如图所示的连接方法，四个8bit数据线构成了32bit数据线。

2. 字扩展：用多片位宽相同的存储器芯片扩展包含更多存储器的过程。就像串联一样。一般是对于每个字的位数足够而字的数目不足的时候使用。字扩展示意图如下：

   ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/word_extend.png)

   如图我们将两片32K 8bit的芯片组合成一块64K 8bit的存储器。存储器最终是64K，因此需要16根地址线（$$2^{16}=65536=64K$$），数据位数仍然是8bit，因此需要8根数据线。但是每个芯片都是32K，因此需要15根地址线。进行级联后第16根地址线$$A_{15}$$需要连接到一个译码器，输出信号即为片选信号。该片选信号选择此时读写的芯片。

3. 例题1，对于8086CPU，用RAM芯片6264组成32KB 存储空间，地址范围为00000H～07FFFH，画出连接控制原理图（用74138译码器产生片选信号）。

   6264芯片是一个8K 8bit的存储芯片。如果要扩展为32K 16bit的存储器，则需要8块存储器。其中两个一组做位扩展，四组一起做字扩展。8086提供了20条地址线，此处总容量为32K，则可以使用15条地址线做寻址，输出是16位，则对应16bit总线。剩余五条总线可以用来分配为片选。

   8086内存组织如下。20条地址线，可以寻址1MB的存储空间，00000H~FFFFFH。一般是两个512KB存储器组成，一个是奇地址存储器（高字节），与数据总线高八位相连；一个是偶地址存储器（低字节），与数据总线8位相连。两个存储器均与$$A_{19}-A_1$$连接。

    ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/8086ram.png)

   以上是8086的内存组织，一般19-1的19条地址总线进入地址锁存器，并由锁存器送入奇偶存储芯片中；$$\overline{BHE}$$和$$A_0$$进行片选，选择奇地址与偶地址对应的芯片。两个各8bit的数据线分别连接到数据总线中。

   我们说构成32KB存储器，就是32K 8bit大小的存储器，实际上可以是16K 16bit存储器，大小是等效的。考虑到8086的结构就是16bit数据线，因此就可以采用16K 16bit方案。此时一共需要四片。

   针对位扩展问题，我们采用两片一组的方式，这时构成8K 16bit的单个存储器；然后对于32KB存储，按照8086的规则应当分解为两个16K 8bit存储器，因此对每组进行字扩展到16K，相当于奇偶地址的存储器各使用2个存储芯片。

   接下来的内容可以有多种理解方式。奇偶存储单元每一侧都需要16K的寻址，因此需要14根地址线。而实际上进入后只有13根地址线进行8K寻址，另一个作为片选信号了。所以也可以认为是13条地址线进行寻址。$$A_0$$用于奇偶片选，用于选择高低位，$$A_{14}$$进行片选，用于选择内存前半部分和后半部分（字扩展片选）。

   总结起来就是13条进行实际寻址，0和14条进行片选。

    ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/eg1.png)

   7FFFH=0111 1111 1111 1111


4. 例题2，对于8086CPU，用RAM芯片6264组成32KB 存储空间，地址范围为50000H～57FFFH，画出连接控制原理图（用门电路产生片选信号） 。

   相比上一个例题，地址发生了变化。

   50000H=0101 0000 0000 0000 0000

   57FFFH=0101 0111 1111 1111 1111

   实际上还是相同的空间，只是最高位发生了改变，总大小不发生改变。看一看答案还是很容易懂的。

    ![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/eg2.png)



## 6. 日常

今天大家在群里给龙神策划生日。提前了一周时间，还邀请了龙神最好最熟悉的朋友们。

龙神说自己喜欢吃小龙虾，我们找遍了中关村的店，也不能在人均两位数解决问题。

在大家最后决定高价解决问题的时候我也是想退出的

最后还是决定为了这个好朋友搏一把。

真羡慕这样的人，能有我们这样的朋友。

二十年了，**从来没有人**能替我策划一次生日，安排surprise。一股莫名的悲伤一下子涌上来了。

**我永远属于那种隐形人物。**

**也不配拥有这样的好朋友。**



**2018年10月27日00:45:06**

## 7. 日常

狗屎微电子工艺实验

2018年10月28日09:31:12

## 8. 日常

雅思发广告的人对我发动了一次道德攻击，要不是有人劝我我都快内疚死了，草。

先给我发了个文件袋，我收下了，然后让我扫码，我不太想扫，但是拿人家东西又不太好，所以还给人家了。

然后他对我说了声**谢谢**

草

这个场面就像是我拒绝了一个在严寒中发小广告的人，还对人家的工作表现出了强烈的不屑以及贪图小利的小人心理

我草草草这个人有点骚

2018-11-10 00:15:17

## 9. 待办备忘

1. 该开始复习了草
2. 通信仿真是个啥玩意
3. **安排个放大器**
4. 

## 10. 如何让耳朵变成耳机的形状

32$$\Omega$$的耳机就这么难出声了，没想到耳机是这么难伺候的东西。

目前感觉是，直推声音没有多少低频分量，声音集中在人声及以上的频段，这就很狗屎了。如果Windows声音控制器是线性的，至少现在低频低3dB-6dB的增益，需要脑放和耳放来补充。这时候已经为信仰充值了，接下来必须要让耳朵变成耳机的形状了，先听一个月。

但是DT880的声音真的是一言难尽，如果为了杂食听歌还不如买HD650。当年最出名的三款2k元级耳机，HD650，DT880，K701，现在只有森海还维持在2K元级，从不降价，DT880已经降到了相对合理的1500元级别，而K701因为难伺候的原因，现在已经降到8-900元了。侧面表现我应该买一个HD650，又好伺候又好听。

2018-11-14 10:50:04

你说这个低阻抗的IE60怎么就这么好听呢。

2018-11-14 11:05:53

## 11. 日常

无事发生（X

2018-11-14 11:08:52

## 12. 日常

耳朵快要被DT880惯坏了。听完了大耳880终于听出来了IE60声音到底哪里不足，某种意义上规正了听音观。DT880的声音是真的好听，适合我这样不那么追求刺激低频的老年人，中高频的解析力真的很强，不过没有什么合适的设备能够较好地驱动。

看了一下价格，DT990已经降价到1300以下了，看风评DT990似乎更均衡一些，不过无妨，他没880好看。

2018-11-18 23:05:30



## 13. 日常

我操这人好贱啊，居然对老实人下手。

大学期间见过最贱的人了。

想起来《人渣的本愿》了，艺术果然取自生活，这回没能高于生活。

2018-11-19 15:30:37

## 14. 数字图像处理课程设计 - Style Transfer

论文1508.06576v2（ArXiv）,A neural algorithm of artistic style,发表于CVPR16时有修改，题目为Image Style Transfer Using Convolutional Neural Networks，内容大体相似。

该算法在2016年时大受推崇，结果直观，理论简介，且容易在各种平台复现。本次采用`TensorFlow`进行复现。

本算法实现的思路为

1. 使用**现成**的**识别网络**，提取图像的不同层级的特征
2. 低层的响应描述了图像的**大体风格**，高层次的响应描述了图像的**内容**。
3. 采用梯度下降方法优化**输入响应**，使得我们在特定层获取特定响应。
4. 经过足够次数的迭代，输入响应就获得了特定的风格与内容。

如果说mnist拥有五层卷积层和三层全连接层只是一个简单的机器学习任务，那不得不说，本项目的实现是依赖了如假包换的**深度学习**。

#### 14.1 VGG

基于深度卷积神经网络的图像识别模型很多，例如GoogleNet、LeNet等，此处选择的VGG模型也是其中的一种。VGG取义`Oxford Visual Geometry Group`，隶属于1985年成立的机器人研究组，这个模型实际上与传统的CNN模型并无较大差异（或者说就是一个较为复杂的CNN模型）。在图像分类比赛中获取过较好的成绩。其模型的结构为

![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/VGG.png)

#### 14.2 响应（内容）逼近

对于每一个网络层，实际上输出结果可以表示为

$$x_{t+1}=f(Wx_t+b)$$

其中网络的权重为$$W$$，$$x_t$$是前一层的输出，$$x_{t+1}$$是当前层的输出。论文中指出，这个池化采用了average_pooling，据说可以略微提升效果。

这个(pretrained) VGG网络是现成的、训练好的，不需要我们做额外的操作就能够在低层级输出图片的小区域的信息（边角、曲线），中间层输出较大尺度的信息（方块、螺旋），高层级输出最大尺度的信息（图片的总体内容信息）

接下来需要调教网络的响应。将这个网络视为一个系统（非线性系统）

**任取一个图像**$$X^0$$，将其输入上述的网络中，第$$l$$卷积层的响应我们记为$$X^l$$，其尺寸为$$H^l \times  W^l \times N^l$$。

即对于输入$$X^0$$，得到系统响应为$$X^l$$。

对于我们的**目标图像**$$\overline{X^0}$$，送入同样的网络，同样可以得到第$$l$$层响应为$$\overline{X^l}$$。

即对于输入$$\overline{X^0}$$，得到系统响应为$$\overline{X^l}$$。

如果我们希望调整系统的输入，使得两个输入变得相似，一个可行的方法就是调整其响应。视**确定风格**的图像的响应为固定值，修改目标图像的响应，就可以让目标图像在某种程度上接近**风格图像的风格**。

这个调整过程是神经网络优化算法中常见的反向传播方法。假设我要调整第$$l$$层的误差，我们就可以设计范数误差函数

$$E_c^l=\frac{1}{2}\mid \mid X^l-\overline{X^l}\mid \mid ^2$$

两个响应的误差如上，想要最小化误差，对该误差函数进行求导，即得

$$\frac{\partial E_c^l}{x^l_{h,w,k}}=x^l_{h,w,k}-\overline{x^l_{h,w,k}}$$

被求导对象是本层的所有元素，$$h$$是本卷积层输出响应的高度，$$w$$是宽度，$$k$$是个数。

用此图辅助理解

![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/mnistCNN.png)

进一步的，运用链式求导法则，将误差反向传播到输入层，整个过程就是反向传播。

*通过第一次求导，可以得出前一层的误差，第二次求导获取更前一层的误差，通过多次的链式求导，就能求得最前一层的误差，我们按照梯度方向进行修正，就能让输入图像的==内容==更加接近。*

#### 14.3 风格逼近

风格是没有位置关系的。

**格拉姆矩阵**（Gram Matrix），协方差矩阵。格拉姆矩阵的定义为

$$\Delta (\alpha_1,\alpha_2,...,\alpha_k)=\left[ \begin{matrix}(\alpha_1,\alpha_1) & (\alpha_1,\alpha_2) & ... & (\alpha_1,\alpha_k)\\ (\alpha_2,\alpha_1) & (\alpha_2,\alpha_2) & ... & (\alpha_2,\alpha_k)\\ ...&...&...&... \\ (\alpha_k,\alpha_1) & (\alpha_k,\alpha_2) & ... & (\alpha_k,\alpha_k)\end{matrix} \right]$$

其中括号表示内积。

实际上可以用来判断特征之间的偏心协方差（未减去均值的协方差矩阵）。论文中采用Gram矩阵为

$$G_{i,j}^l=\sum\limits_{hw}x_{j,w,i}^l · x_{h,w,j}^l$$

即第$$l$$层的格拉姆矩阵用作特征矩阵，由于计算方法抛弃了位置信息，因此可以看做是对风格的描述。第$$i,j$$位置的元素用于描述第$$i$$和第$$j$$个响应的相关性。这可以看作是一个图形风格的代表。

为了让风格像上面的内容一样得到近似，我们采用相似的方法进行逼近。

$$E_c^l=\frac{1}{2}\mid \mid G^l-\overline{G^l}\mid \mid ^2$$

对误差求导可以得到

$$\frac{\partial E_s^l}{x^l_{h,w,k}}=(X^l)^T(G^l-\overline{G^l})_{i,j}$$

同样通过反向传播法则可以使目标图像和我们选取的图像风格逼近。

![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/StyleExperiment.png)

#### 14.4 测试

为了不失一般性，采用高斯噪声作为初始输入图像，选取一幅特殊风格的图像作为参考。其中

代表了内容特征的卷积层选择$$conv4\_2$$

代表了风格特征的卷积层选择$$conv1\_1,conv2\_1,conv3\_1,conv4\_1,conv5\_1$$

设内容误差权重为$$\alpha$$，设风格误差权重为$$\beta$$，则误差函数可以联合为

$$Loss_{total} = \alpha Loss_{content}+\beta Loss_{style}$$

即

$$Loss_{total} = \frac{1}{2}\alpha \mid \mid X^l-\overline{X^l}\mid \mid ^2+\beta \frac{1}{2}\mid \mid G^l-\overline{G^l}\mid \mid ^2$$

以下是训练了100次后的结果，耗时约10分钟。

![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/1542695387.result.png)

对比训练了2000次的结果

![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/LX.jpg)

![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/1620.png)

2018-11-21 15:30:40

## 15. 出分

高级DSP出分了，今天不想学习，只想哭

2018-11-23 17:20:23

## 16. 日常

大学的一半时间过去了。虽然认知、经历在两年之内得到了很大的提升，甚至在性格、处事上也发生了很大的改变，但是对未来的想法却越来越模糊。我从来也不是一个工于计划的人，二十年的人生中，但凡没有他认为我做好的计划，我都是走一步看一步，就像The Beatles歌中的一样，对待凡事的态度都是“let it be”。

每每到了总结与思考的时候，我总是感觉无话可说。在我目前短暂的人生中，没有什么值得一提的伟大经历，也没有什么值得称道的贡献，因此每到此时回头一看自己也只是一个普通人罢了。我的父母常将我与社区里其他人的子女作对比，总是称赞我至今为止的成绩，但是也只有我自己明白，这些所谓的成绩什么也不是。也许人与人之间真的存在天生的差距吧，我们每个人Born this way。在我的记忆中，我从来没有在某件事上做过**特殊的**努力，一切就像安排好的一样，顺水推舟就到了今天，又到了回忆与反思的时间。

 两年前的此时正式入学之初，几位学长带着我们在外面办了个小小的迎接仪式，至少在最后吃饭的时候大家都还很开心。回学校时天色已晚，几位学长带着我们一起回到了宿舍，路上大家开始聊起了人生。那是我第一次感觉到自己的卑微，没有原因，但却挥之不去。走在北京的街头上，我只能感叹自己的出身，让我不得不从高中开始背井离乡的生活。曾经的我一定没想到，这一走竟然走了这么久、这么远。

 两年前的开题，我的梦想是**提升阶层**。当时的我可能是人生中第一次意识到了阶层带给人的影响。这个影响是与生俱来的，也许会受用终生，但却几乎不能被改变的。入学时我曾乐观地认为通过所谓的努力就能实现阶层的进步，但是到今天回看大学这两年的时间，有多少次我付出了努力却仍然什么都没能收获（注：应景的是，此时刚刚出了一个成绩，是我自认为投入很多心血的课程，分数很低），又有多少次明明没有放在心上，却被给予了高度评价。既然努力与收获相关性那么差，那为什么我会认为努力就能改变阶层呢？以我现在的视角来看，改变阶层是一件对于我来说几乎不能完成的任务。

 人受到环境的影响是一件很正常的事情，受到各方面的影响，我的想法也始终在发生改变。截至今日，我的注意力逐渐停留在了一个特殊的身份上，那就是“普通”。这个普通有很多含义，但在我看来最主要的含义还是“外表看起来普通”。我想做一个有杰出贡献的人，无论是对个人、集体，还是对国家，这个想法从来没有发生过改变，但也是在不久前，我才出现了“普通”的想法。与“普通”结合起来，我也许最想要成为一个幕后工作者一样的人物吧。

 出现在公众视野中的人从来也不是我心目中的强者，他们也许在自我包装与展示这方面有自己独特的手法，但是在自己的本职工作中往往表现的不尽如人意。相反，支撑着这整个国家走过了风风雨雨的70年的人，才是值得敬仰并成为目标的人。然而即便如此，我从来没有对这些凭借自己本领赚取维生资本的人给予任何一点的鄙视。相反的我也尊重并羡慕他们。

 看起来目标似乎很明确了，但是实际做起来却感觉什么目标也没有。受到几位十分优秀的学长的影响，从上学期我开始对将来的人生做一个暂时的规划。至今为止我其实对此仍然一无所知，将来我会做一个什么样的人，会从事什么样的工作，即便现在正在为目标准备，我也始终感觉将来的人生是一个未知数。

 现在再提远大的理想抱负就显得十分的无力。远大的理想与抱负什么人都可以拥有，但对未来生活十分肯定的人却很少。我也过了该持有抱负的年龄了，也是时候该计划自己**从来没有机遇**的人生了。

暑假期间我第一次联系了一位学校里的十分优秀的老师（注：计算机学院的宋丹丹老师）。虽然环境特殊，但我此处直言不讳，将来的研究生生涯我仍然更希望攻读计算机方向。过自己想要的生活是每一个人正当的诉求。老师让我做职业规划时我才发现我甚至对自己知之甚少。暑假随学院的第一次美国之行让我第一次有了出国的想法，但事实却是出国需要钱和能力中的至少一种；暑假跟随学长一起做竞赛，在学长的帮助下还是成功的拿到了团队第一名的成绩，我又有了做竞赛的想法，但是做竞赛的同时，保送研究生以及将来报名夏令营时我可能就无法拿出成绩。我向来是厌恶这一类“竞争冒险”的，面临这些问题，我只能说不行。

截止今天，就未来走向问题我已经咨询过很多老师的意见，其中也包括班主任杨老师。每位老师给我的建议我都接受了，但是仍然不能肯定将来走什么样的道路于我而言收益最高，又或者说最为稳定。

咨询了许多学长学姐的留学、保研或外推的经历，我才渐渐真实感觉每个人都是不同的，相同的道路不同的人走也很可能是两种结果。真正适合自己的道路还是需要自己去探索，但这个过程中我们注定都是迷茫的。

我很羡慕选定了目标并可以为目标坚持努力的人，相比之下我这样连目标都没能确定的人实在是没有什么值得骄傲的地方。

说完了迷茫之处也该说一下目前在备选目标上做的一些工作。我入学后没多久的时间，我接触到了关于Machine Learning之类的相关内容。其中的一些算法对于我来说有很强的吸引力，在我见到这些东西的时候，一个声音告诉我，我想从事这方面相关的工作。在德育开题的时候，我也说过希望能走到这个领域。经过两年的调查与了解，我也阅读了这个领域内的几十篇论文，复现过这个领域中的几个著名的模型，在渐渐地了解了这个行业的奥秘之后也想过为这个领域贡献一些自己的力量。

能让我坚持在这个方面学习相关内容，除了我自己的兴趣之外，给我最多帮助的就是几位学长和老师了。我感谢他们每一个人对我的帮助，接受了帮助的我也希望能有一天将这份帮助传播出去。

另外，这里还希望感谢大学期间我遇到的所有人。特殊的机会能让我们相遇，能让我们之间产生各种社会关系与联系，能让我们在彼此的生活中留下自己的痕迹，能让我接收到每一个人的帮助，在此我要感谢所有遇到的人，无论是老师同学还是素不相识的学长学姐。



> Though I know I'll never lose affection
>
> For people and things that went before
>
> I know I'll often stop and think about them
>
> In my life, I love you all.



2018-11-21 22:52:26

北京市 北京理工大学

写下这些东西的几天后，和一位同学简单聊了一下相关内容。从我个人角度来看，人生有很多过法，而我则希望这是一个不断地让自己变强的过程。也许我穷尽一生也难以达到大学接触到的各位老师的水平，但希望自己一步一步变得更为强大的愿景是不会改变的。无论如何，即使没能找到最合适的目标也要继续前进吧，也许再往前走些就能看到路标呢。

 

**仍然感谢我所遇到的每个人。**

 

2018-11-24 11:29:32

北京市 北京理工大学

## 17. 日常

很久没早起过了，突然早起有点受不了了（）

2018-11-25 07:37:14

想买一瓶Whiskey。

2018-11-25 07:40:43



## 18. 日常

在教室看见一袋**牛羊配**，当年我还挺喜欢吃的。。。

现在真是啥都不敢吃了。

2018-11-28 15:11:55

刚刚才发现，目前为止这个博客中已经有将近20万字了。`Typora`统计结果可能略有偏差。

2018-11-28 15:26:44

为什么最近总是出现一重启就不能用网易云音乐和TIM的情况？？

绝了今天忘了去上课结果老师没来。

2018-11-29 10:49:07



## 19. 日常

`2018-11-30 16:09:24`

哈哈哈啥时候才能改掉上台一秒钟出汗一小时的毛病。

`2018-11-30 16:10:00`

哇我居然roll出来了一个秒是**00**的时间

`2018-11-30 16:10:50`

每次上课都要吐槽一句，幸亏我没选`MNIST`，不然这车撞的XD

`2018-11-30 16:45:46`

突然想起来年轻的时候，对于自己的想法总是口无遮拦地大声讲出来，现在总是选择避而不谈地告诉几个关系好的朋友，还总是怕遭遇被刺（

`2018-11-30 18:45:26`

你TM哪来的优越感，jbdxbl，你这种垃圾我从学弟里面都能找几个完爆你的



## 20. 日常

`2018-12-1 09:53:10`

![](https://raw.githubusercontent.com/psycholsc/psycholsc.github.io/master/assets/12.1.png)

小伙汁你的思想很危险。

